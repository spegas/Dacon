{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LG_V1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMS4ceM0xQMugkjm4Id/YGR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spegas/Dacon/blob/main/Get_Price_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3J3S1HAswgc",
        "outputId": "1a9a0fdf-84f7-4d71-bf1c-f9adf7788661"
      },
      "source": [
        "!pip install finance-datareader"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting finance-datareader\n",
            "  Downloading finance_datareader-0.9.31-py3-none-any.whl (17 kB)\n",
            "Collecting requests-file\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from finance-datareader) (4.62.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from finance-datareader) (4.2.6)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from finance-datareader) (2.23.0)\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.7/dist-packages (from finance-datareader) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19.2->finance-datareader) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19.2->finance-datareader) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19.2->finance-datareader) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.19.2->finance-datareader) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->finance-datareader) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->finance-datareader) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->finance-datareader) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->finance-datareader) (3.0.4)\n",
            "Installing collected packages: requests-file, finance-datareader\n",
            "Successfully installed finance-datareader-0.9.31 requests-file-1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDldBr3ttOwb",
        "outputId": "dddceb87-858a-45e6-9993-1bacedf4ed4e"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9672 sha256=0aac2173c28b79e092605c1f3de19f892fa6b78d1fd1a7c2d53fbe7a04fe7ab3\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRl1AiHstGQ5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import FinanceDataReader as fdr\n",
        "import wget\n",
        "\n",
        "from zipfile import ZipFile\n",
        "from os.path import basename"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDcJu5upvpml"
      },
      "source": [
        "if not os.path.isdir(\"base\"):\n",
        "  print(\"base 디렉토리 생성\")\n",
        "  os.makedirs(\"base\")\n",
        "\n",
        "if not os.path.isdir(\"price\"):\n",
        "  print(\"price 디렉토리 생성\")\n",
        "  os.makedirs(\"price\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdtUujcrvwKA",
        "outputId": "820829b7-c2e6-4eea-b557-2bbcdc4f8047"
      },
      "source": [
        "stock_list_file_url = \"https://raw.githubusercontent.com/spegas/Dacon/main/stock_list.csv\"\n",
        "if os.path.exists('base/stock_list.csv'):\n",
        "  os.remove('base/stock_list.csv')\n",
        "  print('이전에 저장된 주식 종목 리스트 파일을 삭제 합니다.')\n",
        "wget.download(stock_list_file_url, 'base/stock_list.csv')\n",
        "print('주식 종목 리스트 파일 다운로드 완료')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "주식 종목 리스트 파일 다운로드 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTXwab-FwKxb"
      },
      "source": [
        "stock_list_name = 'base/stock_list.csv'\n",
        "pd_stock_list = pd.read_csv(stock_list_name)\n",
        "pd_stock_list['종목코드'] = pd_stock_list['종목코드'].apply(lambda x : str(x).zfill(6))\n",
        "pd_stock_list.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMPpBg6Mv0aY"
      },
      "source": [
        "start_date = '20200101'\n",
        "end_date = '20211031'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2ZDm_E6v4wH"
      },
      "source": [
        "for f_index in range(0, pd_stock_list.shape[0]):\n",
        "  f_code = pd_stock_list.iloc[f_index]['종목코드']\n",
        "  f_name = pd_stock_list.iloc[f_index]['종목명']\n",
        "  price_filename = str('price/') + str(f_code) + str('_price.csv')\n",
        "  if not os.path.exists(price_filename):\n",
        "    f_price = fdr.DataReader(f_code, start = start_date, end = end_date)\n",
        "    f_price.to_csv(price_filename)\n",
        "    print('[종목코드 : %s, 종목명 : %s] 가격 정보 저장 완료' % (str(f_code), str(f_name)))\n",
        "  else:\n",
        "    print('[종목코드 : %s, 종목명 : %s] 가격 정보 파일 존재' % (str(f_code), str(f_name)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSSO4jU-6WCd"
      },
      "source": [
        "# Zip the files from given directory that matches the filter\n",
        "def zipFilesInDir(dirName, zipFileName, filter):\n",
        "   # create a ZipFile object\n",
        "   with ZipFile(zipFileName, 'w') as zipObj:\n",
        "       # Iterate over all the files in directory\n",
        "       for folderName, subfolders, filenames in os.walk(dirName):\n",
        "           for filename in filenames:\n",
        "               if filter(filename):\n",
        "                   # create complete filepath of file in directory\n",
        "                   filePath = os.path.join(folderName, filename)\n",
        "                   # Add file to zip\n",
        "                   zipObj.write(filePath, basename(filePath))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lfaKaW_6ZF9"
      },
      "source": [
        "zipFilesInDir('price', 'price.zip', lambda name : 'csv' in name)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2puLLmGC4nh"
      },
      "source": [
        "pd_price = pd.read_csv('price/000670_price.csv')"
      ],
      "execution_count": 307,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "DPJSzVPaDMys",
        "outputId": "263a6159-7669-4017-9d32-8a48f4e6e080"
      },
      "source": [
        "pd_price"
      ],
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Change</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-01-02</td>\n",
              "      <td>645000</td>\n",
              "      <td>653000</td>\n",
              "      <td>632000</td>\n",
              "      <td>646000</td>\n",
              "      <td>1065</td>\n",
              "      <td>0.001550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-01-03</td>\n",
              "      <td>647000</td>\n",
              "      <td>664000</td>\n",
              "      <td>641000</td>\n",
              "      <td>658000</td>\n",
              "      <td>1431</td>\n",
              "      <td>0.018576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-01-06</td>\n",
              "      <td>647000</td>\n",
              "      <td>659000</td>\n",
              "      <td>643000</td>\n",
              "      <td>652000</td>\n",
              "      <td>1018</td>\n",
              "      <td>-0.009119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-01-07</td>\n",
              "      <td>652000</td>\n",
              "      <td>666000</td>\n",
              "      <td>647000</td>\n",
              "      <td>658000</td>\n",
              "      <td>1338</td>\n",
              "      <td>0.009202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-01-08</td>\n",
              "      <td>644000</td>\n",
              "      <td>651000</td>\n",
              "      <td>636000</td>\n",
              "      <td>644000</td>\n",
              "      <td>1336</td>\n",
              "      <td>-0.021277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>447</th>\n",
              "      <td>2021-10-25</td>\n",
              "      <td>710000</td>\n",
              "      <td>729000</td>\n",
              "      <td>710000</td>\n",
              "      <td>727000</td>\n",
              "      <td>1071</td>\n",
              "      <td>0.019635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>448</th>\n",
              "      <td>2021-10-26</td>\n",
              "      <td>730000</td>\n",
              "      <td>740000</td>\n",
              "      <td>728000</td>\n",
              "      <td>733000</td>\n",
              "      <td>1231</td>\n",
              "      <td>0.008253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>449</th>\n",
              "      <td>2021-10-27</td>\n",
              "      <td>733000</td>\n",
              "      <td>734000</td>\n",
              "      <td>716000</td>\n",
              "      <td>721000</td>\n",
              "      <td>984</td>\n",
              "      <td>-0.016371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>450</th>\n",
              "      <td>2021-10-28</td>\n",
              "      <td>721000</td>\n",
              "      <td>722000</td>\n",
              "      <td>696000</td>\n",
              "      <td>697000</td>\n",
              "      <td>1632</td>\n",
              "      <td>-0.033287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>451</th>\n",
              "      <td>2021-10-29</td>\n",
              "      <td>697000</td>\n",
              "      <td>701000</td>\n",
              "      <td>684000</td>\n",
              "      <td>686000</td>\n",
              "      <td>1768</td>\n",
              "      <td>-0.015782</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>452 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Date    Open    High     Low   Close  Volume    Change\n",
              "0    2020-01-02  645000  653000  632000  646000    1065  0.001550\n",
              "1    2020-01-03  647000  664000  641000  658000    1431  0.018576\n",
              "2    2020-01-06  647000  659000  643000  652000    1018 -0.009119\n",
              "3    2020-01-07  652000  666000  647000  658000    1338  0.009202\n",
              "4    2020-01-08  644000  651000  636000  644000    1336 -0.021277\n",
              "..          ...     ...     ...     ...     ...     ...       ...\n",
              "447  2021-10-25  710000  729000  710000  727000    1071  0.019635\n",
              "448  2021-10-26  730000  740000  728000  733000    1231  0.008253\n",
              "449  2021-10-27  733000  734000  716000  721000     984 -0.016371\n",
              "450  2021-10-28  721000  722000  696000  697000    1632 -0.033287\n",
              "451  2021-10-29  697000  701000  684000  686000    1768 -0.015782\n",
              "\n",
              "[452 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 308
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np1ydmhGDPC8"
      },
      "source": [
        "# create 20 days simple moving average column\n",
        "pd_price['20_SMA'] = pd_price['Close'].rolling(window = 20, min_periods = 1).mean()\n",
        "pd_price['50_SMA'] = pd_price['Close'].rolling(window = 50, min_periods = 1).mean()"
      ],
      "execution_count": 309,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XWoUEidDcNL"
      },
      "source": [
        "pd_price['20_EMA'] = round(pd_price['Close'].ewm(span = 20, adjust = False).mean(), 2)\n",
        "pd_price['50_EMA'] = round(pd_price['Close'].ewm(span = 50, adjust = False).mean(), 2)"
      ],
      "execution_count": 310,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Z6XY5xvkEFsq",
        "outputId": "42770533-2b90-4f71-806b-1f7d99df5272"
      },
      "source": [
        "pd_price"
      ],
      "execution_count": 311,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Change</th>\n",
              "      <th>20_SMA</th>\n",
              "      <th>50_SMA</th>\n",
              "      <th>20_EMA</th>\n",
              "      <th>50_EMA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-01-02</td>\n",
              "      <td>645000</td>\n",
              "      <td>653000</td>\n",
              "      <td>632000</td>\n",
              "      <td>646000</td>\n",
              "      <td>1065</td>\n",
              "      <td>0.001550</td>\n",
              "      <td>646000.0</td>\n",
              "      <td>646000.0</td>\n",
              "      <td>646000.00</td>\n",
              "      <td>646000.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-01-03</td>\n",
              "      <td>647000</td>\n",
              "      <td>664000</td>\n",
              "      <td>641000</td>\n",
              "      <td>658000</td>\n",
              "      <td>1431</td>\n",
              "      <td>0.018576</td>\n",
              "      <td>652000.0</td>\n",
              "      <td>652000.0</td>\n",
              "      <td>647142.86</td>\n",
              "      <td>646470.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-01-06</td>\n",
              "      <td>647000</td>\n",
              "      <td>659000</td>\n",
              "      <td>643000</td>\n",
              "      <td>652000</td>\n",
              "      <td>1018</td>\n",
              "      <td>-0.009119</td>\n",
              "      <td>652000.0</td>\n",
              "      <td>652000.0</td>\n",
              "      <td>647605.44</td>\n",
              "      <td>646687.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-01-07</td>\n",
              "      <td>652000</td>\n",
              "      <td>666000</td>\n",
              "      <td>647000</td>\n",
              "      <td>658000</td>\n",
              "      <td>1338</td>\n",
              "      <td>0.009202</td>\n",
              "      <td>653500.0</td>\n",
              "      <td>653500.0</td>\n",
              "      <td>648595.40</td>\n",
              "      <td>647131.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-01-08</td>\n",
              "      <td>644000</td>\n",
              "      <td>651000</td>\n",
              "      <td>636000</td>\n",
              "      <td>644000</td>\n",
              "      <td>1336</td>\n",
              "      <td>-0.021277</td>\n",
              "      <td>651600.0</td>\n",
              "      <td>651600.0</td>\n",
              "      <td>648157.74</td>\n",
              "      <td>647008.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>447</th>\n",
              "      <td>2021-10-25</td>\n",
              "      <td>710000</td>\n",
              "      <td>729000</td>\n",
              "      <td>710000</td>\n",
              "      <td>727000</td>\n",
              "      <td>1071</td>\n",
              "      <td>0.019635</td>\n",
              "      <td>691500.0</td>\n",
              "      <td>696440.0</td>\n",
              "      <td>704409.67</td>\n",
              "      <td>696012.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>448</th>\n",
              "      <td>2021-10-26</td>\n",
              "      <td>730000</td>\n",
              "      <td>740000</td>\n",
              "      <td>728000</td>\n",
              "      <td>733000</td>\n",
              "      <td>1231</td>\n",
              "      <td>0.008253</td>\n",
              "      <td>693350.0</td>\n",
              "      <td>697560.0</td>\n",
              "      <td>707132.56</td>\n",
              "      <td>697463.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>449</th>\n",
              "      <td>2021-10-27</td>\n",
              "      <td>733000</td>\n",
              "      <td>734000</td>\n",
              "      <td>716000</td>\n",
              "      <td>721000</td>\n",
              "      <td>984</td>\n",
              "      <td>-0.016371</td>\n",
              "      <td>694750.0</td>\n",
              "      <td>698360.0</td>\n",
              "      <td>708453.27</td>\n",
              "      <td>698386.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>450</th>\n",
              "      <td>2021-10-28</td>\n",
              "      <td>721000</td>\n",
              "      <td>722000</td>\n",
              "      <td>696000</td>\n",
              "      <td>697000</td>\n",
              "      <td>1632</td>\n",
              "      <td>-0.033287</td>\n",
              "      <td>695500.0</td>\n",
              "      <td>698520.0</td>\n",
              "      <td>707362.48</td>\n",
              "      <td>698331.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>451</th>\n",
              "      <td>2021-10-29</td>\n",
              "      <td>697000</td>\n",
              "      <td>701000</td>\n",
              "      <td>684000</td>\n",
              "      <td>686000</td>\n",
              "      <td>1768</td>\n",
              "      <td>-0.015782</td>\n",
              "      <td>696200.0</td>\n",
              "      <td>698340.0</td>\n",
              "      <td>705327.96</td>\n",
              "      <td>697848.26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>452 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Date    Open    High  ...    50_SMA     20_EMA     50_EMA\n",
              "0    2020-01-02  645000  653000  ...  646000.0  646000.00  646000.00\n",
              "1    2020-01-03  647000  664000  ...  652000.0  647142.86  646470.59\n",
              "2    2020-01-06  647000  659000  ...  652000.0  647605.44  646687.43\n",
              "3    2020-01-07  652000  666000  ...  653500.0  648595.40  647131.06\n",
              "4    2020-01-08  644000  651000  ...  651600.0  648157.74  647008.27\n",
              "..          ...     ...     ...  ...       ...        ...        ...\n",
              "447  2021-10-25  710000  729000  ...  696440.0  704409.67  696012.73\n",
              "448  2021-10-26  730000  740000  ...  697560.0  707132.56  697463.21\n",
              "449  2021-10-27  733000  734000  ...  698360.0  708453.27  698386.22\n",
              "450  2021-10-28  721000  722000  ...  698520.0  707362.48  698331.86\n",
              "451  2021-10-29  697000  701000  ...  698340.0  705327.96  697848.26\n",
              "\n",
              "[452 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 311
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "W3bjvx5rEH07",
        "outputId": "72947eb7-ae1a-406b-a160-55c58a4de399"
      },
      "source": [
        "pd_price.corr()"
      ],
      "execution_count": 312,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Change</th>\n",
              "      <th>20_SMA</th>\n",
              "      <th>50_SMA</th>\n",
              "      <th>20_EMA</th>\n",
              "      <th>50_EMA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Open</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.994695</td>\n",
              "      <td>0.995650</td>\n",
              "      <td>0.990223</td>\n",
              "      <td>-0.070524</td>\n",
              "      <td>-0.048255</td>\n",
              "      <td>0.932445</td>\n",
              "      <td>0.855207</td>\n",
              "      <td>0.957004</td>\n",
              "      <td>0.895326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>High</th>\n",
              "      <td>0.994695</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.993403</td>\n",
              "      <td>0.996150</td>\n",
              "      <td>-0.065004</td>\n",
              "      <td>0.019211</td>\n",
              "      <td>0.929249</td>\n",
              "      <td>0.858987</td>\n",
              "      <td>0.954887</td>\n",
              "      <td>0.897209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Low</th>\n",
              "      <td>0.995650</td>\n",
              "      <td>0.993403</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.994876</td>\n",
              "      <td>-0.077220</td>\n",
              "      <td>0.007712</td>\n",
              "      <td>0.929551</td>\n",
              "      <td>0.850744</td>\n",
              "      <td>0.954046</td>\n",
              "      <td>0.890681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Close</th>\n",
              "      <td>0.990223</td>\n",
              "      <td>0.996150</td>\n",
              "      <td>0.994876</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.070717</td>\n",
              "      <td>0.073685</td>\n",
              "      <td>0.926218</td>\n",
              "      <td>0.853867</td>\n",
              "      <td>0.951661</td>\n",
              "      <td>0.892079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Volume</th>\n",
              "      <td>-0.070524</td>\n",
              "      <td>-0.065004</td>\n",
              "      <td>-0.077220</td>\n",
              "      <td>-0.070717</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.001891</td>\n",
              "      <td>-0.070518</td>\n",
              "      <td>-0.067129</td>\n",
              "      <td>-0.073092</td>\n",
              "      <td>-0.074101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Change</th>\n",
              "      <td>-0.048255</td>\n",
              "      <td>0.019211</td>\n",
              "      <td>0.007712</td>\n",
              "      <td>0.073685</td>\n",
              "      <td>0.001891</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.068082</td>\n",
              "      <td>-0.047012</td>\n",
              "      <td>-0.058058</td>\n",
              "      <td>-0.056427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_SMA</th>\n",
              "      <td>0.932445</td>\n",
              "      <td>0.929249</td>\n",
              "      <td>0.929551</td>\n",
              "      <td>0.926218</td>\n",
              "      <td>-0.070518</td>\n",
              "      <td>-0.068082</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.942389</td>\n",
              "      <td>0.995041</td>\n",
              "      <td>0.971804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50_SMA</th>\n",
              "      <td>0.855207</td>\n",
              "      <td>0.858987</td>\n",
              "      <td>0.850744</td>\n",
              "      <td>0.853867</td>\n",
              "      <td>-0.067129</td>\n",
              "      <td>-0.047012</td>\n",
              "      <td>0.942389</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.946237</td>\n",
              "      <td>0.990910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_EMA</th>\n",
              "      <td>0.957004</td>\n",
              "      <td>0.954887</td>\n",
              "      <td>0.954046</td>\n",
              "      <td>0.951661</td>\n",
              "      <td>-0.073092</td>\n",
              "      <td>-0.058058</td>\n",
              "      <td>0.995041</td>\n",
              "      <td>0.946237</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.974503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50_EMA</th>\n",
              "      <td>0.895326</td>\n",
              "      <td>0.897209</td>\n",
              "      <td>0.890681</td>\n",
              "      <td>0.892079</td>\n",
              "      <td>-0.074101</td>\n",
              "      <td>-0.056427</td>\n",
              "      <td>0.971804</td>\n",
              "      <td>0.990910</td>\n",
              "      <td>0.974503</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Open      High       Low  ...    50_SMA    20_EMA    50_EMA\n",
              "Open    1.000000  0.994695  0.995650  ...  0.855207  0.957004  0.895326\n",
              "High    0.994695  1.000000  0.993403  ...  0.858987  0.954887  0.897209\n",
              "Low     0.995650  0.993403  1.000000  ...  0.850744  0.954046  0.890681\n",
              "Close   0.990223  0.996150  0.994876  ...  0.853867  0.951661  0.892079\n",
              "Volume -0.070524 -0.065004 -0.077220  ... -0.067129 -0.073092 -0.074101\n",
              "Change -0.048255  0.019211  0.007712  ... -0.047012 -0.058058 -0.056427\n",
              "20_SMA  0.932445  0.929249  0.929551  ...  0.942389  0.995041  0.971804\n",
              "50_SMA  0.855207  0.858987  0.850744  ...  1.000000  0.946237  0.990910\n",
              "20_EMA  0.957004  0.954887  0.954046  ...  0.946237  1.000000  0.974503\n",
              "50_EMA  0.895326  0.897209  0.890681  ...  0.990910  0.974503  1.000000\n",
              "\n",
              "[10 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 312
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4DChqkYEX-t"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 313,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyDdgNJlKBhO",
        "outputId": "b380ed18-aa64-4b0e-c2c1-7f624f0ceaf5"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "scale_cols = ['Open', 'High', 'Low', 'Close', 'Volume', '20_SMA', '50_EMA', '20_EMA', '50_EMA']\n",
        "df_scaled = scaler.fit_transform(pd_price[scale_cols])\n",
        "\n",
        "pd_scaled_price = pd.DataFrame(df_scaled)\n",
        "pd_scaled_price.columns = scale_cols\n",
        "\n",
        "print(pd_scaled_price)\n",
        "\n",
        "pd_sel_price = pd_price[scale_cols]"
      ],
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Open      High       Low  ...    50_EMA    20_EMA    50_EMA\n",
            "0    0.688243  0.679452  0.683060  ...  0.753301  0.712312  0.753301\n",
            "1    0.693527  0.709589  0.707650  ...  0.755517  0.717104  0.755517\n",
            "2    0.693527  0.695890  0.713115  ...  0.756538  0.719044  0.756538\n",
            "3    0.706737  0.715068  0.724044  ...  0.758627  0.723195  0.758627\n",
            "4    0.685601  0.673973  0.693989  ...  0.758049  0.721360  0.758049\n",
            "..        ...       ...       ...  ...       ...       ...       ...\n",
            "447  0.859974  0.887671  0.896175  ...  0.988823  0.957247  0.988823\n",
            "448  0.912814  0.917808  0.945355  ...  0.995653  0.968666  0.995653\n",
            "449  0.920740  0.901370  0.912568  ...  1.000000  0.974204  1.000000\n",
            "450  0.889036  0.868493  0.857923  ...  0.999744  0.969630  0.999744\n",
            "451  0.825627  0.810959  0.825137  ...  0.997467  0.961098  0.997467\n",
            "\n",
            "[452 rows x 9 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eZwDvNWK4xY",
        "outputId": "bb60daff-c367-4af4-c6bf-0093758b2025"
      },
      "source": [
        "pd_scaled_price.shape"
      ],
      "execution_count": 315,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(452, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 315
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dFPqZ1-K_ex"
      },
      "source": [
        "def make_dataset(data, label, window_size=20):\n",
        "    feature_list = []\n",
        "    label_list = []\n",
        "    for i in range(len(data) - window_size):\n",
        "        feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
        "        label_list.append(np.array(label.iloc[i+window_size]))\n",
        "    return np.array(feature_list), np.array(label_list)\n"
      ],
      "execution_count": 316,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyPJhA3AK1fn"
      },
      "source": [
        "TEST_SIZE = 350"
      ],
      "execution_count": 317,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt34q9tFKp1Z"
      },
      "source": [
        "#train = pd_scaled_price[:TEST_SIZE]\n",
        "#test = pd_scaled_price[TEST_SIZE:]\n",
        "train = pd_sel_price[:TEST_SIZE]\n",
        "test = pd_sel_price[TEST_SIZE:]"
      ],
      "execution_count": 318,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTRbJxR5K__F",
        "outputId": "d3939517-e7f9-4fbe-c0a1-88a4e634bf43"
      },
      "source": [
        "feature_cols = ['Open', 'High', 'Low', 'Volume', '20_SMA', '50_EMA', '20_EMA', '50_EMA']\n",
        "label_cols = ['Close']\n",
        "\n",
        "train_feature = train[feature_cols]\n",
        "train_label = train[label_cols]\n",
        "\n",
        "# train dataset\n",
        "train_feature, train_label = make_dataset(train_feature, train_label, 50)\n",
        "\n",
        "# train, validation set 생성\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)\n",
        "\n",
        "print(x_train.shape, x_valid.shape)\n",
        "# ((6086, 20, 4), (1522, 20, 4))\n",
        "\n",
        "test_feature = test[feature_cols]\n",
        "test_label = test[label_cols]\n",
        "\n",
        "# test dataset (실제 예측 해볼 데이터)\n",
        "test_feature, test_label = make_dataset(test_feature, test_label, 50)\n",
        "\n",
        "print(test_feature.shape, test_label.shape)\n",
        "# ((180, 20, 4), (180, 1))"
      ],
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(240, 50, 10) (60, 50, 10)\n",
            "(52, 50, 10) (52, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBLtiaNXLNCw"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(5, \n",
        "               input_shape=(train_feature.shape[1], train_feature.shape[2]), \n",
        "               activation='relu', \n",
        "               return_sequences=False)\n",
        "          )\n",
        "model.add(Dense(5))\n",
        "model.add(Dense(1))"
      ],
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OBzOEktDM9Lj",
        "outputId": "063858e3-54ef-45e8-c4de-6445db8e2ce1"
      },
      "source": [
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "#early_stop = EarlyStopping(monitor='val_loss', patience=20)\n",
        "filename = os.path.join('result/', 'tmp_checkpoint.h5')\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=200, \n",
        "                    batch_size=16,\n",
        "                    validation_data=(x_valid, y_valid), \n",
        "                    callbacks=[checkpoint])\n",
        "'''\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=200, \n",
        "                    batch_size=16,\n",
        "                    validation_data=(x_valid, y_valid), \n",
        "                    callbacks=[early_stop, checkpoint])\n",
        "'''"
      ],
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 6408065515520.0000\n",
            "Epoch 00001: val_loss improved from inf to 3504673652736.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 1s 31ms/step - loss: 5462939402240.0000 - val_loss: 3504673652736.0000\n",
            "Epoch 2/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 2137341296640.0000\n",
            "Epoch 00002: val_loss improved from 3504673652736.00000 to 1897937633280.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 2067236388864.0000 - val_loss: 1897937633280.0000\n",
            "Epoch 3/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 1283888119808.0000\n",
            "Epoch 00003: val_loss improved from 1897937633280.00000 to 564984152064.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 1197026967552.0000 - val_loss: 564984152064.0000\n",
            "Epoch 4/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 475383234560.0000\n",
            "Epoch 00004: val_loss improved from 564984152064.00000 to 281690243072.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 433451237376.0000 - val_loss: 281690243072.0000\n",
            "Epoch 5/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 320146145280.0000\n",
            "Epoch 00005: val_loss did not improve from 281690243072.00000\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 320146145280.0000 - val_loss: 283290435584.0000\n",
            "Epoch 6/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 304392044544.0000\n",
            "Epoch 00006: val_loss improved from 281690243072.00000 to 280398168064.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 302068400128.0000 - val_loss: 280398168064.0000\n",
            "Epoch 7/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 300482199552.0000\n",
            "Epoch 00007: val_loss improved from 280398168064.00000 to 279352836096.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 300482199552.0000 - val_loss: 279352836096.0000\n",
            "Epoch 8/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 301301563392.0000\n",
            "Epoch 00008: val_loss improved from 279352836096.00000 to 278798106624.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 300520931328.0000 - val_loss: 278798106624.0000\n",
            "Epoch 9/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 299850924032.0000\n",
            "Epoch 00009: val_loss improved from 278798106624.00000 to 278400761856.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 299850924032.0000 - val_loss: 278400761856.0000\n",
            "Epoch 10/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 300171362304.0000\n",
            "Epoch 00010: val_loss improved from 278400761856.00000 to 277999878144.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 299396825088.0000 - val_loss: 277999878144.0000\n",
            "Epoch 11/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 296179564544.0000\n",
            "Epoch 00011: val_loss improved from 277999878144.00000 to 277786361856.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 298908286976.0000 - val_loss: 277786361856.0000\n",
            "Epoch 12/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 303127298048.0000\n",
            "Epoch 00012: val_loss improved from 277786361856.00000 to 277608300544.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 298703421440.0000 - val_loss: 277608300544.0000\n",
            "Epoch 13/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 294990577664.0000\n",
            "Epoch 00013: val_loss improved from 277608300544.00000 to 277530214400.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 298490429440.0000 - val_loss: 277530214400.0000\n",
            "Epoch 14/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 295755022336.0000\n",
            "Epoch 00014: val_loss improved from 277530214400.00000 to 277473820672.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 298393829376.0000 - val_loss: 277473820672.0000\n",
            "Epoch 15/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 299559059456.0000\n",
            "Epoch 00015: val_loss improved from 277473820672.00000 to 277434892288.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 298306469888.0000 - val_loss: 277434892288.0000\n",
            "Epoch 16/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 298248896512.0000\n",
            "Epoch 00016: val_loss improved from 277434892288.00000 to 277391212544.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 298248896512.0000 - val_loss: 277391212544.0000\n",
            "Epoch 17/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 299572953088.0000\n",
            "Epoch 00017: val_loss improved from 277391212544.00000 to 277343305728.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 298193616896.0000 - val_loss: 277343305728.0000\n",
            "Epoch 18/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 299881758720.0000\n",
            "Epoch 00018: val_loss improved from 277343305728.00000 to 277311225856.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 298111598592.0000 - val_loss: 277311225856.0000\n",
            "Epoch 19/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 301045514240.0000\n",
            "Epoch 00019: val_loss improved from 277311225856.00000 to 277268922368.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 298190667776.0000 - val_loss: 277268922368.0000\n",
            "Epoch 20/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 297997107200.0000\n",
            "Epoch 00020: val_loss improved from 277268922368.00000 to 277236350976.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 298105470976.0000 - val_loss: 277236350976.0000\n",
            "Epoch 21/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 296673771520.0000\n",
            "Epoch 00021: val_loss improved from 277236350976.00000 to 277196275712.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 298051436544.0000 - val_loss: 277196275712.0000\n",
            "Epoch 22/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 299521146880.0000\n",
            "Epoch 00022: val_loss improved from 277196275712.00000 to 277165834240.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297979084800.0000 - val_loss: 277165834240.0000\n",
            "Epoch 23/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 292696129536.0000\n",
            "Epoch 00023: val_loss improved from 277165834240.00000 to 277136572416.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 297927376896.0000 - val_loss: 277136572416.0000\n",
            "Epoch 24/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 297127444480.0000\n",
            "Epoch 00024: val_loss improved from 277136572416.00000 to 277116747776.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297884352512.0000 - val_loss: 277116747776.0000\n",
            "Epoch 25/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 296799272960.0000\n",
            "Epoch 00025: val_loss improved from 277116747776.00000 to 277103509504.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297853845504.0000 - val_loss: 277103509504.0000\n",
            "Epoch 26/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 297014165504.0000\n",
            "Epoch 00026: val_loss improved from 277103509504.00000 to 277090369536.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297828777984.0000 - val_loss: 277090369536.0000\n",
            "Epoch 27/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 297619488768.0000\n",
            "Epoch 00027: val_loss improved from 277090369536.00000 to 277073952768.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297807740928.0000 - val_loss: 277073952768.0000\n",
            "Epoch 28/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 297222144000.0000\n",
            "Epoch 00028: val_loss improved from 277073952768.00000 to 277060648960.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297777037312.0000 - val_loss: 277060648960.0000\n",
            "Epoch 29/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 295340638208.0000\n",
            "Epoch 00029: val_loss improved from 277060648960.00000 to 277046001664.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297751773184.0000 - val_loss: 277046001664.0000\n",
            "Epoch 30/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 291834822656.0000\n",
            "Epoch 00030: val_loss improved from 277046001664.00000 to 277029257216.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297727852544.0000 - val_loss: 277029257216.0000\n",
            "Epoch 31/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297696395264.0000\n",
            "Epoch 00031: val_loss improved from 277029257216.00000 to 277014872064.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297696395264.0000 - val_loss: 277014872064.0000\n",
            "Epoch 32/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 294696288256.0000\n",
            "Epoch 00032: val_loss improved from 277014872064.00000 to 277000552448.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297668280320.0000 - val_loss: 277000552448.0000\n",
            "Epoch 33/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 297946939392.0000\n",
            "Epoch 00033: val_loss improved from 277000552448.00000 to 276985708544.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297643737088.0000 - val_loss: 276985708544.0000\n",
            "Epoch 34/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 299494244352.0000\n",
            "Epoch 00034: val_loss improved from 276985708544.00000 to 276972142592.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297621454848.0000 - val_loss: 276972142592.0000\n",
            "Epoch 35/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 292508958720.0000\n",
            "Epoch 00035: val_loss improved from 276972142592.00000 to 276959363072.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297600647168.0000 - val_loss: 276959363072.0000\n",
            "Epoch 36/200\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 300180144128.0000\n",
            "Epoch 00036: val_loss improved from 276959363072.00000 to 276954775552.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 297585803264.0000 - val_loss: 276954775552.0000\n",
            "Epoch 37/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 295443333120.0000\n",
            "Epoch 00037: val_loss improved from 276954775552.00000 to 276953104384.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297580331008.0000 - val_loss: 276953104384.0000\n",
            "Epoch 38/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297578496000.0000\n",
            "Epoch 00038: val_loss improved from 276953104384.00000 to 276953071616.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297578496000.0000 - val_loss: 276953071616.0000\n",
            "Epoch 39/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 298703618048.0000\n",
            "Epoch 00039: val_loss improved from 276953071616.00000 to 276953006080.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297577283584.0000 - val_loss: 276953006080.0000\n",
            "Epoch 40/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 305853464576.0000\n",
            "Epoch 00040: val_loss improved from 276953006080.00000 to 276952973312.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297576660992.0000 - val_loss: 276952973312.0000\n",
            "Epoch 41/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297575546880.0000\n",
            "Epoch 00041: val_loss improved from 276952973312.00000 to 276952907776.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297575546880.0000 - val_loss: 276952907776.0000\n",
            "Epoch 42/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 299308711936.0000\n",
            "Epoch 00042: val_loss improved from 276952907776.00000 to 276952875008.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 297574694912.0000 - val_loss: 276952875008.0000\n",
            "Epoch 43/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297573875712.0000\n",
            "Epoch 00043: val_loss did not improve from 276952875008.00000\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 297573875712.0000 - val_loss: 276952875008.0000\n",
            "Epoch 44/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297572827136.0000\n",
            "Epoch 00044: val_loss improved from 276952875008.00000 to 276952809472.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297572827136.0000 - val_loss: 276952809472.0000\n",
            "Epoch 45/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 300075679744.0000\n",
            "Epoch 00045: val_loss improved from 276952809472.00000 to 276952743936.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297572040704.0000 - val_loss: 276952743936.0000\n",
            "Epoch 46/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297571188736.0000\n",
            "Epoch 00046: val_loss improved from 276952743936.00000 to 276952711168.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297571188736.0000 - val_loss: 276952711168.0000\n",
            "Epoch 47/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297569943552.0000\n",
            "Epoch 00047: val_loss improved from 276952711168.00000 to 276952645632.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297569943552.0000 - val_loss: 276952645632.0000\n",
            "Epoch 48/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 303957475328.0000\n",
            "Epoch 00048: val_loss improved from 276952645632.00000 to 276952612864.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297569157120.0000 - val_loss: 276952612864.0000\n",
            "Epoch 49/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 292643405824.0000\n",
            "Epoch 00049: val_loss improved from 276952612864.00000 to 276952580096.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 297568075776.0000 - val_loss: 276952580096.0000\n",
            "Epoch 50/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297567092736.0000\n",
            "Epoch 00050: val_loss improved from 276952580096.00000 to 276952547328.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297567092736.0000 - val_loss: 276952547328.0000\n",
            "Epoch 51/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297566044160.0000\n",
            "Epoch 00051: val_loss improved from 276952547328.00000 to 276952481792.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297566044160.0000 - val_loss: 276952481792.0000\n",
            "Epoch 52/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 295699185664.0000\n",
            "Epoch 00052: val_loss improved from 276952481792.00000 to 276952449024.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 297564995584.0000 - val_loss: 276952449024.0000\n",
            "Epoch 53/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 296244969472.0000\n",
            "Epoch 00053: val_loss improved from 276952449024.00000 to 276952416256.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297564012544.0000 - val_loss: 276952416256.0000\n",
            "Epoch 54/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 294714212352.0000\n",
            "Epoch 00054: val_loss improved from 276952416256.00000 to 276952350720.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297563226112.0000 - val_loss: 276952350720.0000\n",
            "Epoch 55/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 298515005440.0000\n",
            "Epoch 00055: val_loss improved from 276952350720.00000 to 276952285184.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297561948160.0000 - val_loss: 276952285184.0000\n",
            "Epoch 56/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297560735744.0000\n",
            "Epoch 00056: val_loss did not improve from 276952285184.00000\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 297560735744.0000 - val_loss: 276952285184.0000\n",
            "Epoch 57/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 297151332352.0000\n",
            "Epoch 00057: val_loss improved from 276952285184.00000 to 276952186880.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297559851008.0000 - val_loss: 276952186880.0000\n",
            "Epoch 58/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 294865174528.0000\n",
            "Epoch 00058: val_loss improved from 276952186880.00000 to 276952154112.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 297558671360.0000 - val_loss: 276952154112.0000\n",
            "Epoch 59/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297557557248.0000\n",
            "Epoch 00059: val_loss improved from 276952154112.00000 to 276952121344.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297557557248.0000 - val_loss: 276952121344.0000\n",
            "Epoch 60/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 296178810880.0000\n",
            "Epoch 00060: val_loss improved from 276952121344.00000 to 276952088576.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297556410368.0000 - val_loss: 276952088576.0000\n",
            "Epoch 61/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297555558400.0000\n",
            "Epoch 00061: val_loss improved from 276952088576.00000 to 276952023040.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297555558400.0000 - val_loss: 276952023040.0000\n",
            "Epoch 62/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297555525632.0000\n",
            "Epoch 00062: val_loss improved from 276952023040.00000 to 276951957504.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297555525632.0000 - val_loss: 276951957504.0000\n",
            "Epoch 63/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 300452872192.0000\n",
            "Epoch 00063: val_loss improved from 276951957504.00000 to 276951924736.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 297555460096.0000 - val_loss: 276951924736.0000\n",
            "Epoch 64/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 297236856832.0000\n",
            "Epoch 00064: val_loss improved from 276951924736.00000 to 276951891968.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297555394560.0000 - val_loss: 276951891968.0000\n",
            "Epoch 65/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 301564133376.0000\n",
            "Epoch 00065: val_loss improved from 276951891968.00000 to 276951826432.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297555394560.0000 - val_loss: 276951826432.0000\n",
            "Epoch 66/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 301771816960.0000\n",
            "Epoch 00066: val_loss did not improve from 276951826432.00000\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 297555329024.0000 - val_loss: 276951826432.0000\n",
            "Epoch 67/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 302326448128.0000\n",
            "Epoch 00067: val_loss improved from 276951826432.00000 to 276951760896.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297555296256.0000 - val_loss: 276951760896.0000\n",
            "Epoch 68/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 294798655488.0000\n",
            "Epoch 00068: val_loss improved from 276951760896.00000 to 276951662592.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297555263488.0000 - val_loss: 276951662592.0000\n",
            "Epoch 69/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 301232914432.0000\n",
            "Epoch 00069: val_loss improved from 276951662592.00000 to 276951629824.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297555132416.0000 - val_loss: 276951629824.0000\n",
            "Epoch 70/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297555132416.0000\n",
            "Epoch 00070: val_loss improved from 276951629824.00000 to 276951597056.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297555132416.0000 - val_loss: 276951597056.0000\n",
            "Epoch 71/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297555066880.0000\n",
            "Epoch 00071: val_loss improved from 276951597056.00000 to 276951564288.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297555066880.0000 - val_loss: 276951564288.0000\n",
            "Epoch 72/200\n",
            "10/15 [===================>..........] - ETA: 0s - loss: 296665055232.0000\n",
            "Epoch 00072: val_loss improved from 276951564288.00000 to 276951498752.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297555034112.0000 - val_loss: 276951498752.0000\n",
            "Epoch 73/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297555001344.0000\n",
            "Epoch 00073: val_loss improved from 276951498752.00000 to 276951433216.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297555001344.0000 - val_loss: 276951433216.0000\n",
            "Epoch 74/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 302617165824.0000\n",
            "Epoch 00074: val_loss did not improve from 276951433216.00000\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 297554968576.0000 - val_loss: 276951433216.0000\n",
            "Epoch 75/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297554903040.0000\n",
            "Epoch 00075: val_loss improved from 276951433216.00000 to 276951367680.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297554903040.0000 - val_loss: 276951367680.0000\n",
            "Epoch 76/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 297101033472.0000\n",
            "Epoch 00076: val_loss improved from 276951367680.00000 to 276951302144.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297554837504.0000 - val_loss: 276951302144.0000\n",
            "Epoch 77/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297554804736.0000\n",
            "Epoch 00077: val_loss improved from 276951302144.00000 to 276951269376.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297554804736.0000 - val_loss: 276951269376.0000\n",
            "Epoch 78/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297554739200.0000\n",
            "Epoch 00078: val_loss improved from 276951269376.00000 to 276951171072.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297554739200.0000 - val_loss: 276951171072.0000\n",
            "Epoch 79/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 299631837184.0000\n",
            "Epoch 00079: val_loss did not improve from 276951171072.00000\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297554640896.0000 - val_loss: 276951171072.0000\n",
            "Epoch 80/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 295995146240.0000\n",
            "Epoch 00080: val_loss improved from 276951171072.00000 to 276951105536.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297554640896.0000 - val_loss: 276951105536.0000\n",
            "Epoch 81/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 298616422400.0000\n",
            "Epoch 00081: val_loss did not improve from 276951105536.00000\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297554575360.0000 - val_loss: 276951105536.0000\n",
            "Epoch 82/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297554542592.0000\n",
            "Epoch 00082: val_loss improved from 276951105536.00000 to 276951040000.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297554542592.0000 - val_loss: 276951040000.0000\n",
            "Epoch 83/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297554477056.0000\n",
            "Epoch 00083: val_loss improved from 276951040000.00000 to 276950974464.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297554477056.0000 - val_loss: 276950974464.0000\n",
            "Epoch 84/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 293884100608.0000\n",
            "Epoch 00084: val_loss improved from 276950974464.00000 to 276950908928.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297554444288.0000 - val_loss: 276950908928.0000\n",
            "Epoch 85/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 297996025856.0000\n",
            "Epoch 00085: val_loss improved from 276950908928.00000 to 276950876160.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 297554411520.0000 - val_loss: 276950876160.0000\n",
            "Epoch 86/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297554345984.0000\n",
            "Epoch 00086: val_loss improved from 276950876160.00000 to 276950843392.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297554345984.0000 - val_loss: 276950843392.0000\n",
            "Epoch 87/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297554280448.0000\n",
            "Epoch 00087: val_loss improved from 276950843392.00000 to 276950777856.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297554280448.0000 - val_loss: 276950777856.0000\n",
            "Epoch 88/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 300683493376.0000\n",
            "Epoch 00088: val_loss improved from 276950777856.00000 to 276950712320.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297554280448.0000 - val_loss: 276950712320.0000\n",
            "Epoch 89/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297554182144.0000\n",
            "Epoch 00089: val_loss did not improve from 276950712320.00000\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297554182144.0000 - val_loss: 276950712320.0000\n",
            "Epoch 90/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 296081227776.0000\n",
            "Epoch 00090: val_loss improved from 276950712320.00000 to 276950646784.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297554116608.0000 - val_loss: 276950646784.0000\n",
            "Epoch 91/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297554051072.0000\n",
            "Epoch 00091: val_loss improved from 276950646784.00000 to 276950581248.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297554051072.0000 - val_loss: 276950581248.0000\n",
            "Epoch 92/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297554018304.0000\n",
            "Epoch 00092: val_loss improved from 276950581248.00000 to 276950548480.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297554018304.0000 - val_loss: 276950548480.0000\n",
            "Epoch 93/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 297304293376.0000\n",
            "Epoch 00093: val_loss improved from 276950548480.00000 to 276950482944.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297554018304.0000 - val_loss: 276950482944.0000\n",
            "Epoch 94/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 299175444480.0000\n",
            "Epoch 00094: val_loss improved from 276950482944.00000 to 276950450176.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297553920000.0000 - val_loss: 276950450176.0000\n",
            "Epoch 95/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 295330119680.0000\n",
            "Epoch 00095: val_loss improved from 276950450176.00000 to 276950351872.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297553854464.0000 - val_loss: 276950351872.0000\n",
            "Epoch 96/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297553854464.0000\n",
            "Epoch 00096: val_loss did not improve from 276950351872.00000\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297553854464.0000 - val_loss: 276950351872.0000\n",
            "Epoch 97/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 295796867072.0000\n",
            "Epoch 00097: val_loss improved from 276950351872.00000 to 276950319104.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297553788928.0000 - val_loss: 276950319104.0000\n",
            "Epoch 98/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 300071583744.0000\n",
            "Epoch 00098: val_loss improved from 276950319104.00000 to 276950253568.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297553723392.0000 - val_loss: 276950253568.0000\n",
            "Epoch 99/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 296947941376.0000\n",
            "Epoch 00099: val_loss improved from 276950253568.00000 to 276950188032.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297553690624.0000 - val_loss: 276950188032.0000\n",
            "Epoch 100/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297553657856.0000\n",
            "Epoch 00100: val_loss improved from 276950188032.00000 to 276950089728.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297553657856.0000 - val_loss: 276950089728.0000\n",
            "Epoch 101/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 297761636352.0000\n",
            "Epoch 00101: val_loss did not improve from 276950089728.00000\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 297553592320.0000 - val_loss: 276950089728.0000\n",
            "Epoch 102/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297553494016.0000\n",
            "Epoch 00102: val_loss improved from 276950089728.00000 to 276950056960.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297553494016.0000 - val_loss: 276950056960.0000\n",
            "Epoch 103/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 302197538816.0000\n",
            "Epoch 00103: val_loss improved from 276950056960.00000 to 276949991424.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297553461248.0000 - val_loss: 276949991424.0000\n",
            "Epoch 104/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 298471161856.0000\n",
            "Epoch 00104: val_loss improved from 276949991424.00000 to 276949925888.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297553428480.0000 - val_loss: 276949925888.0000\n",
            "Epoch 105/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297553362944.0000\n",
            "Epoch 00105: val_loss did not improve from 276949925888.00000\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297553362944.0000 - val_loss: 276949925888.0000\n",
            "Epoch 106/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297553330176.0000\n",
            "Epoch 00106: val_loss improved from 276949925888.00000 to 276949860352.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297553330176.0000 - val_loss: 276949860352.0000\n",
            "Epoch 107/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 298899439616.0000\n",
            "Epoch 00107: val_loss improved from 276949860352.00000 to 276949794816.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297553264640.0000 - val_loss: 276949794816.0000\n",
            "Epoch 108/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297553231872.0000\n",
            "Epoch 00108: val_loss improved from 276949794816.00000 to 276949762048.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297553231872.0000 - val_loss: 276949762048.0000\n",
            "Epoch 109/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297553166336.0000\n",
            "Epoch 00109: val_loss improved from 276949762048.00000 to 276949729280.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297553166336.0000 - val_loss: 276949729280.0000\n",
            "Epoch 110/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297553133568.0000\n",
            "Epoch 00110: val_loss improved from 276949729280.00000 to 276949696512.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297553133568.0000 - val_loss: 276949696512.0000\n",
            "Epoch 111/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 299387289600.0000\n",
            "Epoch 00111: val_loss improved from 276949696512.00000 to 276949598208.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297553035264.0000 - val_loss: 276949598208.0000\n",
            "Epoch 112/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297553002496.0000\n",
            "Epoch 00112: val_loss improved from 276949598208.00000 to 276949565440.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297553002496.0000 - val_loss: 276949565440.0000\n",
            "Epoch 113/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297552936960.0000\n",
            "Epoch 00113: val_loss improved from 276949565440.00000 to 276949499904.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297552936960.0000 - val_loss: 276949499904.0000\n",
            "Epoch 114/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297552936960.0000\n",
            "Epoch 00114: val_loss did not improve from 276949499904.00000\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297552936960.0000 - val_loss: 276949499904.0000\n",
            "Epoch 115/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297552871424.0000\n",
            "Epoch 00115: val_loss improved from 276949499904.00000 to 276949401600.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297552871424.0000 - val_loss: 276949401600.0000\n",
            "Epoch 116/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297552805888.0000\n",
            "Epoch 00116: val_loss improved from 276949401600.00000 to 276949336064.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297552805888.0000 - val_loss: 276949336064.0000\n",
            "Epoch 117/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297552773120.0000\n",
            "Epoch 00117: val_loss improved from 276949336064.00000 to 276949270528.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297552773120.0000 - val_loss: 276949270528.0000\n",
            "Epoch 118/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297552707584.0000\n",
            "Epoch 00118: val_loss did not improve from 276949270528.00000\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297552707584.0000 - val_loss: 276949270528.0000\n",
            "Epoch 119/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 297836675072.0000\n",
            "Epoch 00119: val_loss improved from 276949270528.00000 to 276949204992.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297552642048.0000 - val_loss: 276949204992.0000\n",
            "Epoch 120/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297552609280.0000\n",
            "Epoch 00120: val_loss improved from 276949204992.00000 to 276949139456.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297552609280.0000 - val_loss: 276949139456.0000\n",
            "Epoch 121/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297552543744.0000\n",
            "Epoch 00121: val_loss improved from 276949139456.00000 to 276949073920.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297552543744.0000 - val_loss: 276949073920.0000\n",
            "Epoch 122/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297552478208.0000\n",
            "Epoch 00122: val_loss did not improve from 276949073920.00000\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297552478208.0000 - val_loss: 276949073920.0000\n",
            "Epoch 123/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 296667611136.0000\n",
            "Epoch 00123: val_loss improved from 276949073920.00000 to 276949008384.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297552445440.0000 - val_loss: 276949008384.0000\n",
            "Epoch 124/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 300217729024.0000\n",
            "Epoch 00124: val_loss improved from 276949008384.00000 to 276948942848.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297552412672.0000 - val_loss: 276948942848.0000\n",
            "Epoch 125/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 299269226496.0000\n",
            "Epoch 00125: val_loss improved from 276948942848.00000 to 276948910080.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297552379904.0000 - val_loss: 276948910080.0000\n",
            "Epoch 126/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 295869448192.0000\n",
            "Epoch 00126: val_loss improved from 276948910080.00000 to 276948877312.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297552281600.0000 - val_loss: 276948877312.0000\n",
            "Epoch 127/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297552248832.0000\n",
            "Epoch 00127: val_loss improved from 276948877312.00000 to 276948811776.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297552248832.0000 - val_loss: 276948811776.0000\n",
            "Epoch 128/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297552183296.0000\n",
            "Epoch 00128: val_loss improved from 276948811776.00000 to 276948746240.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297552183296.0000 - val_loss: 276948746240.0000\n",
            "Epoch 129/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297552150528.0000\n",
            "Epoch 00129: val_loss improved from 276948746240.00000 to 276948713472.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297552150528.0000 - val_loss: 276948713472.0000\n",
            "Epoch 130/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 296307064832.0000\n",
            "Epoch 00130: val_loss improved from 276948713472.00000 to 276948680704.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297552084992.0000 - val_loss: 276948680704.0000\n",
            "Epoch 131/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 297485303808.0000\n",
            "Epoch 00131: val_loss improved from 276948680704.00000 to 276948615168.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 297552019456.0000 - val_loss: 276948615168.0000\n",
            "Epoch 132/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297551986688.0000\n",
            "Epoch 00132: val_loss improved from 276948615168.00000 to 276948549632.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297551986688.0000 - val_loss: 276948549632.0000\n",
            "Epoch 133/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 297052897280.0000\n",
            "Epoch 00133: val_loss improved from 276948549632.00000 to 276948484096.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297551921152.0000 - val_loss: 276948484096.0000\n",
            "Epoch 134/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 295680573440.0000\n",
            "Epoch 00134: val_loss did not improve from 276948484096.00000\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297551888384.0000 - val_loss: 276948484096.0000\n",
            "Epoch 135/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 295651049472.0000\n",
            "Epoch 00135: val_loss improved from 276948484096.00000 to 276948418560.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297551790080.0000 - val_loss: 276948418560.0000\n",
            "Epoch 136/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297551757312.0000\n",
            "Epoch 00136: val_loss improved from 276948418560.00000 to 276948353024.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297551757312.0000 - val_loss: 276948353024.0000\n",
            "Epoch 137/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297551724544.0000\n",
            "Epoch 00137: val_loss improved from 276948353024.00000 to 276948287488.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297551724544.0000 - val_loss: 276948287488.0000\n",
            "Epoch 138/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 297926492160.0000\n",
            "Epoch 00138: val_loss improved from 276948287488.00000 to 276948254720.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297551691776.0000 - val_loss: 276948254720.0000\n",
            "Epoch 139/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 298867064832.0000\n",
            "Epoch 00139: val_loss improved from 276948254720.00000 to 276948221952.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 297551659008.0000 - val_loss: 276948221952.0000\n",
            "Epoch 140/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297551560704.0000\n",
            "Epoch 00140: val_loss improved from 276948221952.00000 to 276948156416.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297551560704.0000 - val_loss: 276948156416.0000\n",
            "Epoch 141/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297551560704.0000\n",
            "Epoch 00141: val_loss improved from 276948156416.00000 to 276948090880.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297551560704.0000 - val_loss: 276948090880.0000\n",
            "Epoch 142/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 296328560640.0000\n",
            "Epoch 00142: val_loss improved from 276948090880.00000 to 276948025344.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297551462400.0000 - val_loss: 276948025344.0000\n",
            "Epoch 143/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297551364096.0000\n",
            "Epoch 00143: val_loss improved from 276948025344.00000 to 276947992576.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297551364096.0000 - val_loss: 276947992576.0000\n",
            "Epoch 144/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297551364096.0000\n",
            "Epoch 00144: val_loss improved from 276947992576.00000 to 276947959808.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297551364096.0000 - val_loss: 276947959808.0000\n",
            "Epoch 145/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 299256807424.0000\n",
            "Epoch 00145: val_loss improved from 276947959808.00000 to 276947894272.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297551331328.0000 - val_loss: 276947894272.0000\n",
            "Epoch 146/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 297289940992.0000\n",
            "Epoch 00146: val_loss improved from 276947894272.00000 to 276947828736.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297551265792.0000 - val_loss: 276947828736.0000\n",
            "Epoch 147/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 295957069824.0000\n",
            "Epoch 00147: val_loss did not improve from 276947828736.00000\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297551200256.0000 - val_loss: 276947828736.0000\n",
            "Epoch 148/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 298986536960.0000\n",
            "Epoch 00148: val_loss improved from 276947828736.00000 to 276947763200.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297551167488.0000 - val_loss: 276947763200.0000\n",
            "Epoch 149/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 294792822784.0000\n",
            "Epoch 00149: val_loss improved from 276947763200.00000 to 276947697664.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297551101952.0000 - val_loss: 276947697664.0000\n",
            "Epoch 150/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 297047228416.0000\n",
            "Epoch 00150: val_loss improved from 276947697664.00000 to 276947632128.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297551101952.0000 - val_loss: 276947632128.0000\n",
            "Epoch 151/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297551036416.0000\n",
            "Epoch 00151: val_loss improved from 276947632128.00000 to 276947599360.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297551036416.0000 - val_loss: 276947599360.0000\n",
            "Epoch 152/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 298050289664.0000\n",
            "Epoch 00152: val_loss improved from 276947599360.00000 to 276947566592.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 297550905344.0000 - val_loss: 276947566592.0000\n",
            "Epoch 153/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 297327951872.0000\n",
            "Epoch 00153: val_loss improved from 276947566592.00000 to 276947501056.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 297550872576.0000 - val_loss: 276947501056.0000\n",
            "Epoch 154/200\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 298263019520.0000\n",
            "Epoch 00154: val_loss improved from 276947501056.00000 to 276947435520.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 297550839808.0000 - val_loss: 276947435520.0000\n",
            "Epoch 155/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 296288288768.0000\n",
            "Epoch 00155: val_loss improved from 276947435520.00000 to 276947369984.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 297550774272.0000 - val_loss: 276947369984.0000\n",
            "Epoch 156/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 297442934784.0000\n",
            "Epoch 00156: val_loss improved from 276947369984.00000 to 276947337216.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297550741504.0000 - val_loss: 276947337216.0000\n",
            "Epoch 157/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 295044710400.0000\n",
            "Epoch 00157: val_loss improved from 276947337216.00000 to 276947304448.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 297550675968.0000 - val_loss: 276947304448.0000\n",
            "Epoch 158/200\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 294531334144.0000\n",
            "Epoch 00158: val_loss improved from 276947304448.00000 to 276947238912.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 297550610432.0000 - val_loss: 276947238912.0000\n",
            "Epoch 159/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 296429617152.0000\n",
            "Epoch 00159: val_loss improved from 276947238912.00000 to 276947173376.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 297550610432.0000 - val_loss: 276947173376.0000\n",
            "Epoch 160/200\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 303728492544.0000\n",
            "Epoch 00160: val_loss did not improve from 276947173376.00000\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297550544896.0000 - val_loss: 276947173376.0000\n",
            "Epoch 161/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 297549824000.0000\n",
            "Epoch 00161: val_loss improved from 276947173376.00000 to 276947140608.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 22ms/step - loss: 297550512128.0000 - val_loss: 276947140608.0000\n",
            "Epoch 162/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297550413824.0000\n",
            "Epoch 00162: val_loss improved from 276947140608.00000 to 276947075072.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297550413824.0000 - val_loss: 276947075072.0000\n",
            "Epoch 163/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 294458458112.0000\n",
            "Epoch 00163: val_loss improved from 276947075072.00000 to 276946976768.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297550348288.0000 - val_loss: 276946976768.0000\n",
            "Epoch 164/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297550315520.0000\n",
            "Epoch 00164: val_loss improved from 276946976768.00000 to 276946944000.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297550315520.0000 - val_loss: 276946944000.0000\n",
            "Epoch 165/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 298885382144.0000\n",
            "Epoch 00165: val_loss improved from 276946944000.00000 to 276946911232.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297550282752.0000 - val_loss: 276946911232.0000\n",
            "Epoch 166/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 298677534720.0000\n",
            "Epoch 00166: val_loss improved from 276946911232.00000 to 276946845696.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 297550217216.0000 - val_loss: 276946845696.0000\n",
            "Epoch 167/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 296167538688.0000\n",
            "Epoch 00167: val_loss improved from 276946845696.00000 to 276946780160.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 297550184448.0000 - val_loss: 276946780160.0000\n",
            "Epoch 168/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297550086144.0000\n",
            "Epoch 00168: val_loss improved from 276946780160.00000 to 276946714624.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 297550086144.0000 - val_loss: 276946714624.0000\n",
            "Epoch 169/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 294649298944.0000\n",
            "Epoch 00169: val_loss improved from 276946714624.00000 to 276946681856.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297550053376.0000 - val_loss: 276946681856.0000\n",
            "Epoch 170/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 300273500160.0000\n",
            "Epoch 00170: val_loss improved from 276946681856.00000 to 276946649088.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297550020608.0000 - val_loss: 276946649088.0000\n",
            "Epoch 171/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 295736967168.0000\n",
            "Epoch 00171: val_loss improved from 276946649088.00000 to 276946616320.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297549922304.0000 - val_loss: 276946616320.0000\n",
            "Epoch 172/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 297473540096.0000\n",
            "Epoch 00172: val_loss improved from 276946616320.00000 to 276946550784.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297549922304.0000 - val_loss: 276946550784.0000\n",
            "Epoch 173/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 298015719424.0000\n",
            "Epoch 00173: val_loss improved from 276946550784.00000 to 276946518016.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297549856768.0000 - val_loss: 276946518016.0000\n",
            "Epoch 174/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 300219170816.0000\n",
            "Epoch 00174: val_loss improved from 276946518016.00000 to 276946452480.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 297549758464.0000 - val_loss: 276946452480.0000\n",
            "Epoch 175/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 296723349504.0000\n",
            "Epoch 00175: val_loss improved from 276946452480.00000 to 276946386944.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 297549725696.0000 - val_loss: 276946386944.0000\n",
            "Epoch 176/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 297348071424.0000\n",
            "Epoch 00176: val_loss improved from 276946386944.00000 to 276946321408.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 297549692928.0000 - val_loss: 276946321408.0000\n",
            "Epoch 177/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 294357630976.0000\n",
            "Epoch 00177: val_loss improved from 276946321408.00000 to 276946288640.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297549660160.0000 - val_loss: 276946288640.0000\n",
            "Epoch 178/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 296112226304.0000\n",
            "Epoch 00178: val_loss improved from 276946288640.00000 to 276946223104.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 297549594624.0000 - val_loss: 276946223104.0000\n",
            "Epoch 179/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297549529088.0000\n",
            "Epoch 00179: val_loss improved from 276946223104.00000 to 276946190336.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297549529088.0000 - val_loss: 276946190336.0000\n",
            "Epoch 180/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297549529088.0000\n",
            "Epoch 00180: val_loss improved from 276946190336.00000 to 276946124800.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297549529088.0000 - val_loss: 276946124800.0000\n",
            "Epoch 181/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 296655585280.0000\n",
            "Epoch 00181: val_loss improved from 276946124800.00000 to 276946092032.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297549463552.0000 - val_loss: 276946092032.0000\n",
            "Epoch 182/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 298513825792.0000\n",
            "Epoch 00182: val_loss improved from 276946092032.00000 to 276946059264.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297549365248.0000 - val_loss: 276946059264.0000\n",
            "Epoch 183/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 298925129728.0000\n",
            "Epoch 00183: val_loss improved from 276946059264.00000 to 276945993728.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297549299712.0000 - val_loss: 276945993728.0000\n",
            "Epoch 184/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297549266944.0000\n",
            "Epoch 00184: val_loss improved from 276945993728.00000 to 276945928192.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297549266944.0000 - val_loss: 276945928192.0000\n",
            "Epoch 185/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297549234176.0000\n",
            "Epoch 00185: val_loss improved from 276945928192.00000 to 276945862656.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297549234176.0000 - val_loss: 276945862656.0000\n",
            "Epoch 186/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 295992983552.0000\n",
            "Epoch 00186: val_loss improved from 276945862656.00000 to 276945829888.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297549168640.0000 - val_loss: 276945829888.0000\n",
            "Epoch 187/200\n",
            "15/15 [==============================] - ETA: 0s - loss: 297549103104.0000\n",
            "Epoch 00187: val_loss improved from 276945829888.00000 to 276945797120.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297549103104.0000 - val_loss: 276945797120.0000\n",
            "Epoch 188/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 297582657536.0000\n",
            "Epoch 00188: val_loss improved from 276945797120.00000 to 276945731584.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 297549070336.0000 - val_loss: 276945731584.0000\n",
            "Epoch 189/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 299931435008.0000\n",
            "Epoch 00189: val_loss improved from 276945731584.00000 to 276945666048.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297549037568.0000 - val_loss: 276945666048.0000\n",
            "Epoch 190/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 296874115072.0000\n",
            "Epoch 00190: val_loss improved from 276945666048.00000 to 276945633280.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 297548972032.0000 - val_loss: 276945633280.0000\n",
            "Epoch 191/200\n",
            "12/15 [=======================>......] - ETA: 0s - loss: 300049203200.0000\n",
            "Epoch 00191: val_loss improved from 276945633280.00000 to 276945600512.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 297548873728.0000 - val_loss: 276945600512.0000\n",
            "Epoch 192/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 298676027392.0000\n",
            "Epoch 00192: val_loss improved from 276945600512.00000 to 276945534976.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297548840960.0000 - val_loss: 276945534976.0000\n",
            "Epoch 193/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 295647707136.0000\n",
            "Epoch 00193: val_loss improved from 276945534976.00000 to 276945502208.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297548808192.0000 - val_loss: 276945502208.0000\n",
            "Epoch 194/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 298020208640.0000\n",
            "Epoch 00194: val_loss improved from 276945502208.00000 to 276945436672.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297548742656.0000 - val_loss: 276945436672.0000\n",
            "Epoch 195/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 297370124288.0000\n",
            "Epoch 00195: val_loss improved from 276945436672.00000 to 276945338368.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297548677120.0000 - val_loss: 276945338368.0000\n",
            "Epoch 196/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 297427337216.0000\n",
            "Epoch 00196: val_loss did not improve from 276945338368.00000\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 297548644352.0000 - val_loss: 276945338368.0000\n",
            "Epoch 197/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 295804108800.0000\n",
            "Epoch 00197: val_loss improved from 276945338368.00000 to 276945305600.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 297548578816.0000 - val_loss: 276945305600.0000\n",
            "Epoch 198/200\n",
            "11/15 [=====================>........] - ETA: 0s - loss: 295335559168.0000\n",
            "Epoch 00198: val_loss improved from 276945305600.00000 to 276945207296.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 297548546048.0000 - val_loss: 276945207296.0000\n",
            "Epoch 199/200\n",
            "13/15 [=========================>....] - ETA: 0s - loss: 298167238656.0000\n",
            "Epoch 00199: val_loss improved from 276945207296.00000 to 276945174528.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297548513280.0000 - val_loss: 276945174528.0000\n",
            "Epoch 200/200\n",
            "14/15 [===========================>..] - ETA: 0s - loss: 298115497984.0000\n",
            "Epoch 00200: val_loss improved from 276945174528.00000 to 276945141760.00000, saving model to result/tmp_checkpoint.h5\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 297548447744.0000 - val_loss: 276945141760.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nhistory = model.fit(x_train, y_train,\\n                    epochs=200, \\n                    batch_size=16,\\n                    validation_data=(x_valid, y_valid), \\n                    callbacks=[early_stop, checkpoint])\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 321
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAzyZ66jNSyn"
      },
      "source": [
        "# weight 로딩\n",
        "model.load_weights(filename)\n",
        "\n",
        "# 예측\n",
        "pred = model.predict(test_feature)"
      ],
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq_MDTV8Rw5h"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 323,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "2d2A4wUJNTlw",
        "outputId": "3d387b68-7df1-45a4-ef4b-5ee22656fa2b"
      },
      "source": [
        "plt.figure(figsize=(12, 9))\n",
        "plt.plot(test_label, label='actual')\n",
        "plt.plot(pred, label='prediction')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtgAAAIICAYAAABU7ZkqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXxU1cH/8c8hKwkhkA3ZEyAQdoSwCaiACq4oWkVFERFUXOrT1q19frXV2lprtWJFS2V1QS1KRasisgiyhQSQNUCAEMKSfQ9Z5/z+yMgDliXADZPl+3695pU7525nApn5zrnnnmOstYiIiIiIiDMaeboCIiIiIiL1iQK2iIiIiIiDFLBFRERERBykgC0iIiIi4iAFbBERERERBylgi4iIiIg4yNvTFXBaWFiYjYyM9HQ1RERERKQeS0hIyLTWhp9qXb0L2JGRkcTHx3u6GiIiIiJSjxljDpxunbqIiIiIiIg4SAFbRERERMRBCtgiIiIiIg6qd32wT6W8vJzU1FRKSko8XZV6w9/fnzZt2uDj4+PpqoiIiIjUKg0iYKemphIUFERkZCTGGE9Xp86z1pKVlUVqaipRUVGero6IiIhIrdIguoiUlJQQGhqqcO0QYwyhoaG6IiAiIiJyCg0iYAMK1w7T71NERETk1BpMwK4rVqxYwZo1ay7oGE2aNHGoNiIiIiJyrhSwaxknAraIiIiIeI4C9kVy8803069fP7p3786MGTMA+Prrr+nbty+9e/dm5MiRJCcn8/bbb/Paa6/Rp08fVq1axX333ceCBQuOH+fH1unCwkJGjhxJ37596dmzJ5999plHXpeIiIiInKxBjCJyot9/vp0dh/MdPWa3Vk157sbuZ9xm1qxZhISEcOzYMfr378+YMWOYPHkyK1euJCoqiuzsbEJCQnjooYdo0qQJv/rVrwCYOXPmKY/n7+/PwoULadq0KZmZmQwaNIibbrpJfaNFREREPKzBBWxPmTZtGgsXLgTg4MGDzJgxg8svv/z4MHchISHndDxrLb/+9a9ZuXIljRo14tChQ6SlpXHJJZc4XncRERERqb4GF7DP1tJcE1asWMG3337L2rVrCQgI4Morr6RPnz4kJiaedV9vb29cLhcALpeLsrIyAN5//30yMjJISEjAx8eHyMhIDZsnIiIiUguoD/ZFkJeXR/PmzQkICCAxMZF169ZRUlLCypUr2b9/PwDZ2dkABAUFUVBQcHzfyMhIEhISAFi0aBHl5eXHjxkREYGPjw/Lly/nwIEDF/lViYiIiMipKGBfBKNHj6aiooKuXbvyzDPPMGjQIMLDw5kxYwZjx46ld+/e3HHHHQDceOONLFy48PhNjpMnT+a7776jd+/erF27lsDAQADuvvtu4uPj6dmzJ/PmzSMmJsaTL1FERERE3Iy11tN1cFRsbKyNj48/qWznzp107drVQzWqv/R7FRERkYbKGJNgrY091Tq1YIuIiIiIOEgBW0RERETEQQrYIiIiIiIOanDD9InIyay17M0oZOnOdJYlppOSXczgDqGM6BrB5Z3Daerv4+kqioiI1CkK2CINUGlFJev3ZbMs8f9CNUDMJUFc2q4Zy3al8+mmQ3g3MvSPDGFETAQjukbQISxQs4WKiIichQK2SAORnl/C8l1VgXrVnkyKyyrx827EkE5hTLm8A8NjImjdrDEAFZUuNh/MZWliOst2pvPilzt58cudRIYGMDwmgpExLRgQFYKvt3qZiYiI/JQCdh20YsUKXnnlFb744gsWLVrEjh07eOaZZ065bW5uLh988AFTp04F4PDhwzz++OMsWLDgYlZZPKSotIJ3Vu1naWIaW1LzAGgZ7M8tl7ZmZNcIBncIo7Gv13/t5+3ViNjIEGIjQ3h6dAypOcUsT0xnaWI6769PYfbqZJr4eTO0UxgDO4QQ3NiHAF9vmvh5E+jnRaCfN4F+3jTx9SbAzwsfLwVxERFpOBSwa5HKykq8vP477JzJTTfdxE033XTa9bm5uUyfPv14wG7VqpXCdQPy4pc7mR+XwqVtm/HkqC6MiIkg5pKgc+7m0aZ5APcMjuSewZEUl1WwJimLpYnpLE9M5+vtR8+6v693I5r4eRPg60V0RBN+c303OkU0Od+XJSIiUqspYF8kycnJjB49mn79+rFx40a6d+/OvHnz6NatG3fccQdLlizhqaeeIiQkhOeee47S0lI6duzI7NmzadKkCV9//TVPPPEEAQEBDB069Phx58yZQ3x8PH//+99JS0vjoYceYt++fQC89dZbTJs2jb1799KnTx+uvvpqHnnkEW644Qa2bdtGSUkJDz/8MPHx8Xh7e/Pqq68yfPhw5syZw6JFiyguLmbv3r3ccsstvPzyy5761cl5Sjyaz4dxKdx3WSTP3djdseMG+HpzVbcWXNWtBdZaMgpLKS6tpLC0guKySopKK9zLFRSWVj0vKquo+llaybLEdK57fRWPj+zElMs7qpuJiIjUOw0vYH/1DBzd6uwxL+kJ17501s127drFzJkzGTJkCPfffz/Tp08HIDQ0lI0bN5KZmcnYsWP59ttvCQwM5M9//jOvvvoqTz31FJMnT2bZsmV06tTp+LTqP/X4449zxRVXsHDhQiorKyksLOSll15i27ZtbN68GagK+j968803McawdetWEhMTueaaa9i9ezcAmzdvZtOmTfj5+dGlSxcee+wx2rZte4G/KLlYrLW8+J+dBPn78POR0TV2HmMMEUH+EFT9fdILSvj95zt45ZvdfLHlCC/d2os+bZvVWB1FREQuNjUdXURt27ZlyJAhAIwfP57vv/8e4HhgXrduHTt27GDIkCH06dOHuXPncuDAARITE4mKiiI6OhpjDOPHjz/l8ZctW8bDDz8MgJeXF8HBwWesz/fff3/8WDExMbRv3/54wB45ciTBwcH4+/vTrVs3Dhw4cOG/ALloVuzOYNWeTH4+MppmAb6ers5JIoL8efOuvvzz3lhyi8u5Zfpqnv98B0WlFZ6umoiIiCMaXgt2NVqaa8pP+73++DwwMBCoanW8+uqrmT9//knb/dj6fDH5+fkdX/by8qKiQuGnriivdPHif3YSFRbI+EHtPV2d07q6WwsGdQjh5a93MWv1fhZvP8qLt/Tgyi4Rnq6aiIjIBVEL9kWUkpLC2rVrAfjggw9O6ksNMGjQIFavXk1SUhIARUVF7N69m5iYGJKTk9m7dy/AfwXwH40cOZK33noLqLphMi8vj6CgIAoKCk65/bBhw3j//fcB2L17NykpKXTp0uXCX6h41IdxKSSlF/LstTG1vn9zkL8PL9zcgwUPDcbfpxH3zd7A/3y0meyiMk9XTUQaqMLSCmas3EtyZpGnqyJ1WO3+9K1nunTpwptvvknXrl3Jyck53p3jR+Hh4cyZM4c777yTXr16MXjwYBITE/H392fGjBlcf/319O3bl4iIU7fwvf766yxfvpyePXvSr18/duzYQWhoKEOGDKFHjx48+eSTJ20/depUXC4XPXv25I477mDOnDkntVxL3ZN3rJzXvt3DoA4hXN2thaerU22xkSF8+fNhPD4ymi+2HOaqV7/j35sOYa31dNUuyA8Hc0lKP/UXXBGpfdILShg3Yy1//DKRa15byV8WJ1Jcpiu4cu5MXf8A+6nY2FgbHx9/UtnOnTvp2rWrh2pUJTk5+fjoHfVFbfi9ysn+9OVOZqzax+ePDqVH6zP3wa+tdh0t4JlPt7ApJZfLO4fz2xu60jG8SZ2bQfKjDSk8++lWXBb6tW/OuP5tuaFXq1OOOy4inrc3o5AJs+LIKizjxVt6sGpPJgs3HaJlsD+/ub4r1/dsWefeh6RmGWMSrLWxp1rX8Ppgi9RTKVnFzF6dzG1929TZcA3Q5ZIgFjx0Ge+uTeblxbu46tWVNAvwoUerYLq3bkrP1sH0aBVM+9CAWvth986qffzhPzu5vHM4wzqFMX9DCk8u2MLzn+9gzKWtGNe/XZ3+NxKpbxIOZDNpbjxexvDhlEH0btuMsX3bcNfAdjz32XYe/WAT73dI4Xc3dafLJecwbJI0WGrBlvOm32vtMvX9BJYnZrDiyStp0dTf09VxxJG8Yyzdmc72w3lsPZTHrqMFlFdWvWcF+XvTo1UwPVo3pUfrYHq0DiYqNJBGjTwXuq21vLZkN9OWJXF9z5a8dkcffL0bYa1lQ3IOH8al8J+tRyitcNGjdVPG9W/HmD6tCPL38VidRRq6xduP8vj8TbQM9mfu/QNoHxp40vpKl+WDuBReWbyLwtIKJgyO5Imro2naAP9uyytdbEjOpqzCxRWdw2ttI8fFcqYWbAVsOW/6vdYeG5Kz+dnba/mfqzrz86tqbtxrTyurcLE7rYBth6oC97bD+ew8kk9ZhQuAQF8vxlzamhfG9MDrIgdtl8vy/Bc7mLMmmTti2/LHsT1PWYe84nL+vfkQ8+NSSDxaQGMfL27o1ZJxA9rRt12zBv+BJXIxvbs2mecWbadnm2bMmhBLaJPT34eUXVTGK9/sYn5cCqGBvjw1Oobb+rbx6Jf6iyGrsJQVuzJYlpjOyt0ZFLiHVB0QGcLvbupOt1ZNPVxDz1HA3rmTmJgYfXA5yFpLYmKiAnYt4HJZbpm+mrT8Upb/6soG18e3vNJFUnoh2w7lsXZfFp9uPMSdA9rxx1t6XLS/+YpKF099soVPNx7igaFR/Ob6rmc9t7WWH1Lz+DAuhUU/HKa4rJIuLYKIjWxORJA/EU39iAjyIyLInxZN/Qht4nfRvzSI1Fcul+Xlxbt4+7u9jIyJ4I27LiXAt3q9ZrcdyuO3n21jY0oufdo24/kx3enVpv5MlmWtZeeRApYlprE0MZ3NB3OxFiKC/BgRE8HwmAiyi8r4y+Jd5BaXcffA9vzyms61bs6Fi6HBB+z9+/cTFBREaGioQrYDrLVkZWVRUFBAVFSUp6vT4C3clMr/fPQDr97em7F923i6Oh73l8WJvLl8L48M78iTo2Jq/Hwl5ZU8Pn8T3+xI45dXd+bREZ3O+X2msLSCz384zCcJqezLLDrlMIWNDIQ2+TF0+9GiqT8RTf0Z1b0F3VupP7dIdZVVuHj6ky0s3FT1ZfyFMd3x9jq3QdVcLsvCTYf401eJZBWVMq5/W/73+m4E+tXNW9uOlVWyZm8mSxPTWZ6YzpG8EgB6twlmREwLRsRE0L1V05Na6/OKy3nt293MW5tMcGMfnhwVwx392zaohoAGH7DLy8tJTU2lpKTEQ7Wqf/z9/WnTpg0+Pg2vD1ptcqyskhF/XUFYEz8+e2RIvb9UWR3WWn69cBvz41L43+u78sCwDjV2rqLSCqa8G8/qpCx+f1N3JlwW6chxyypcZBaWkpZfQnpBKekFpWTkl5CWX0p6wf+VZRaWYi1c1bUFj4/sVK9a0URqQkFJOQ+/t5HvkzLP+wvxT483bekeZq1OpmvLIGbd15+IoLpzD0xJeSWvLdnNnDXJlFa4CPT1Ylh0OCNiIrgyJrxar2XnkXyeW7SduP3Z9GwdzO9u6k6/9s0vQu09r8EHbJH66o2le/jrkt18/OBgBkSFeLo6tUaly/LY/I18ufUor/ysN7f1c75lP7e4jIlzNrAlNY+Xb+3FrTVwjrPJO1bO3DXJzPx+P3nHyhneJZzHR0ZzabuG8eEmci7S8ku4b/YG9qQV8KexPflZbFvHjr08MZ2p728kLMiXORMH0DG8iWPHrinbD+fxi49+YFdaAWP7tuaWS1szICoEP+9z72ZorWXRD4f545c7Scsv5da+bXj62i516svG+VDAFqmH0vJLGP7KCq7oHM5b4/t5ujq1TmlFJZPmxLN2XxZvj+/n6MQ76QUl3Dszjn0ZRbxx16WM6n6JY8c+HwUl5cxbe4B/rtpHbnE5l3cO5+cjO9Gvvb50iQDsSSvgvtkbyCku463x/biic7jj5/jhYC73z9mAy1remdC/1rbiVrosb3+3l799u5tmAb68fFsvhnc59QR256qotIK/L0/inVX78Pf24udXRTPhskh8zrELTl2hgC1SDz214AcWbjrEt7+44r+GlZIqRaUV3PXOenYeyWfe/QMY1CH0go95MLuY8TPXk1FQyj/vjWVIpzAHauqMwtIK3nUH7eyiMoZ0CuXxEdEMdOB1i9RF+SXlzFuTzD9W7sPP24s5E/vX6Bj0B7KKmDArjiN5Jbxx56Vc4+Ev3z+VnFnEL//1AwkHcri+Z0v+cHMPmgc6f3PivoxCfv/5Dr7bnUF0RBP+cHOPevk+pIAtUs9sP5zHDW987x6xopunq1OrZReVcfs/1pKWV8L8KYMu6MM1Kb2A8e/EUVxWwZz7B9C3lnbFKC6r4P11Kfxj5V4yC8sYGBXCz6+KZnAH3egtDUNecTmz1+xn1vf7yS+pYGRMBL+7qTttQwJq/NxZhaVMmhvPltRcfj+mB/cMal/j5zwba6vG8n7xPzvxbmR44eYe3NS7VY2+H1hr+XZnOs9/sZ20vFLmTxlY766qKWCL1CPWWu52t8queHI4wY11o+nZHMk7xm1vraW0opJ/PXQZUWHn1uK//XAeM7/fz+c/HKZZgC/vThpAzCW1f+zXY2WVzI9L4e3v9pJeUEq/9s3p3KL6s9D5eTdi0tCoixJKRJyQU1TGrNX7mbM6mYLSCq7p1oLHR0Zf9JlTj5VV8tj8jXy7M52pV3bkyVFdzivMlle6WJaYzsYDOXRvHcygqBAiznEisfT8Ep76ZAsrdmUwtFMYf/lZL1oGNz7nupyvnKIybpm+moKSCv79yJB69X6igC1SjyzZkcbkefE8P6Y79w6O9HR16oy9GYX87O21NPbx4pOHL+OS4DN/SLlclmWJ6cz8fj9r92UR4OvF7bFtefCKDhf1w8kJJeWVfLThIHPXJlNQUlHt/fKOldOssQ/vPTDwnIK5yMWWXVTGO6v2MXdNMkVllVzX8xIeHR7t0UlQKipd/HbRdj5Yn8LYvq15aWwvfL2r1xc5JauYDzek8K+EVDIKSjEGfoxrUWGBDIwKYWCHEAZGhdKq2enfj/6z5Qi/+fdWSsorefbartwzqL1HRpvam1HILW+upkVTfz6Zelm9mQVTAVukniircDHqbytpZODrJy6vtzeO1JStqXmMm7GW1s0b8/GDg085MUJxWQWfJKQye3Uy+zKLaBXsz31DIrmjf7sGd7Vg19EC7pm5nrJKF3MnDqB3Ww0DKLVLZmEp/1y5j3fXHeBYeSXX92zJYyOi6XJJ7fhCaK3lzeVJvPLNboZFhzH97r4EnSZcllZU8s32ND7ckMLqpCwaGRjeJYJxA9oxLDqMXUcLWL8/i/X7solLzj7+ZbltSGMGRoUyMCqEQR1CaRsSQF5xOb9dtI3PNh+md5tgXr2jj8dHNlmTlMm9s+IY0imMmRNiz3ns8dpIAVukHigpr+QXH2/my61HmXVfLCNinBsVoyFZszeT+2ZtoHvrprz/wMDjs7cdzSth7tpkPlifQt6xcnq3CWbSsA5c2+OSBv1FJiWrmLtnriO7sIx3JvRncMf6d6OS1D3pBSXM+G4f760/QFmFi5t6t+LREZ3oFFE7gvVP/Sv+IM9+upXOLYKYM7H/Sd08ktIL+TAuhU83HSK7qIzWzRpzR/+2/Cy2zWmvllW6LDuP5LN+fzbr92URl5xNbnE5AK2bNaas0kV2URmPjejEI8M71Zr3sA/jUnjm061MGNye34/p4enqXDAFbJE6Lre4jCnzEohLzq7xyVMagq+3HWXq+wkM6RTGL67uzLy1B/j8h8O4rGVU90uYNDSKfu2b64ZAt7T8Esa/s54D2cVMv6svVzk45KHIuVqxK51H3t9ISYWLMX1a8cjwTh5vna2O73Zn8PB7CTQP8OUf9/Rj19ECPtyQwobkHLwbGa7u1oJxA9oxtFPYOc+G6HJZdqcXsH5fNuv3Z5FbXM7To2Nq5VWnF/+zg3+u2u/o5FyeooAtUocdyj3GhFlxpGQV88rtvbmpdytPV6le+HjDQZ76ZAsAgb5e3NG/HROHRNarG3CclFNUxoTZcWw/nM+rt/dmTJ/Wnq6SNEAfu1uCu7QI4s27+57zDcuetu1QHvfN3kBmYSlQ1Z/6jv5tubVvG8KD/Dxcu4uj0mV58N0EliWmMeu+/lzp0BjcnqCALVJH7Ticz32z4zhWXsmMe2J1ed5hCzelkl1Uzs9i29Sbm25qUkFJOZPnxbN+fzbP15Lhx6RhsNbyxrIkXl1y9r7Mtd3B7GLeXXeA4V0iGNQhpEFeKSsqreC2t9dyMLuYTx6+rNb0mT9XCtgiddD3ezJ56L0Egvy9mTNxQJ19A5L6paS8kkc/qBp+7KnRXZh6ZSdPV0nquYpKF//vs23Mjzt4zqNxSO11OPcYN7+5Gl/vRvz7kSGENal7LfhnCthn/R9qjOlijNl8wiPfGPOEMSbEGLPEGLPH/bO5e3tjjJlmjEkyxmwxxvQ94VgT3NvvMcZMOKG8nzFmq3ufacb9de505xCp7xZuSuW+2XG0btaYT6fW3W/3Uv/4+3jx1vh+jOnTipe/3sVLXyVS3xpqpPYoLqvgwXcTmB93kEeGd+SvP+utcF1PtGrWmHcmxJJZWMqUefGUlFd6ukqOOuv/UmvtLmttH2ttH6AfUAwsBJ4Bllpro4Gl7ucA1wLR7scU4C2oCsvAc8BAYADw3AmB+S1g8gn7jXaXn+4cIvWStZbpK5L4n49+IDayOR8/NLjOjbks9Z+PVyNeu70P4we14+3v9vKbf2+j0qWQLc7KLCzlzn+uZ/mudF64uQdPjoppkN0p6rNebZrx6u192JiSy9OfbKlXX9bP9WvgSGCvtfYAMAaY6y6fC9zsXh4DzLNV1gHNjDEtgVHAEmtttrU2B1gCjHava2qtXWerfrPzfnKsU51DpN6pdFl++9l2Xv56Fzf2bsXc+wc0uHGXpe5o1MjwwpgeTL2yIx+sT+GJjzZTXunydLWknkjOLOLWt9aQeCSft8f3U3//euy6ni15clQXPtt8mDeWJXm6Oo7xPsftxwHz3cstrLVH3MtHgR/HbWoNHDxhn1R32ZnKU09RfqZziNQrJeWVPD5/E9/sSOPByzvw9OgYj8y2JXIujDE8NTqGIH8f/vx1IgUl5Uy4LJIOYYG0bta4XkwkIRff5oO5TJqzAZe1fDB5EP3aq3dofTf1yo7szSjk1SW7iQoL5MZ6MFpWtQO2McYXuAl49qfrrLXWGFOj7fpnOocxZgpV3VFo165dTVZDxHE5RWVMmruBTQdzee7GbkwcEuXpKomck4ev7EjTxt789rPtrNiVAYCPl6FdSABRYU3oEB5IZGggUWGBdAgPJCLIT5f65ZSW7kzj0Q82ERbky9yJA+hQB8a3lgtnjOFPY3uSmn2MX/7rB1o1a1znv1idSwv2tcBGa22a+3maMaaltfaIu5tHurv8END2hP3auMsOAVf+pHyFu7zNKbY/0zlOYq2dAcyAqlFEzuE1eYS1lrJKF37eXp6uinhYen4J4/65jtScY0y/qy/X9mzp6SqJnJe7B7bnuh4tScooZH9mUdUjo+rnyj0ZlFX8X/eRAF8vosLcgTsskMjjy00IDlC3qIZqflwKv1m4le6tgpl5XywRQf5n30nqDT9vL96+px+3TF/NuBlrGT+oPY+NiCYk0NfTVTsv1R6mzxjzIbDYWjvb/fwvQJa19iVjzDNAiLX2KWPM9cCjwHVU3dA4zVo7wH2TYwLw46giG4F+1tpsY0wc8DiwHvgSeMNa++XpznGmetbWYfqKyypYnZTFssR0liemczS/hNbNGhMZFuD+oGlCB/eHTJvmurTaEJRWVDJuxjp2HS1gzsQBDIgK8XSVRGqEy2U5nHeM/ZlFJGcWsc8dwPdlFJGaU8yJ90eGBPoeD98nPiJDA2nsq0aJ+ii3uIy3vtvLP77bxxWdw5l+d18C/c61B6vUF+kFJbz6zW4+jj9IoK83D13ZkfuHRNXKv/8LHgfbGBMIpAAdrLV57rJQ4GOgHXAAuN0dlg3wd6pGAikGJlpr49373A/82n3YF08I67HAHKAx8BXwmLtLyCnPcaa61qaAnZpTzPLEdJYmprNmbxZlFS4Cfb24vHM40S2COJhdXPVBk1FIfknF8f28G/14adX9wRIWyKAOoXSK0KWy+sJay1MLtvCvhFTeulst19JwlVW4SMkudrd6F7I/s9j9s4i0/NKTto0MDWDK5R25PbaNGiHqOGst6/Zl8+GGFL7adpSyChe3x7bhxVt64qN/WwH2pBXw8uJdLNmRRoumfvzPVZ25rV/t+tvXRDMXSaXLsiklh6WJ6Szbmc6utAIA2ocGMDKmBSNiIhgQFfJfY3haa8kpLmd/ZiH73JdUk7OqWneSs4ooKXfh592IdycNVCtnPTFn9X5+9/kOHh8ZzS+u7uzp6ojUSoWlFST/2N0ks4jlu9LZlJJLx/BAnhodwzXdWqgvdx2TUVDKgoRUPtqQQnJWMUH+3txyaWvG9W9Ht1ZNPV09qYU2JGfzpy93sjEll04RTXhqVBeuriV/+wrYNWzt3iw+jj/I8l3p5BaX493I0D8yhJFdIxgeE0GHsMDz/o/gcllSsou5f+4GMgpK+WjKYL0J1XFrkjK5Z1YcI2Ii+Mf4fhotRKSarLV8syONP3+dyL6MIvq1b86z18YQG6mGh9qs0mVZtSeDD+MO8u3ONCpclgGRIYwb0JbrerbE36f2XfqX2sVay+Ltaby8uOpvP7Z9c569LoZ+7T37t6+AXcPeWbWP6Sv2cmWXcEbERDAsOtzx8YsP5x7jtrfWUFZpWfDQYCLDAh09vlwcB7OLufHv3xPexI9Pp15GkL9u6BI5VxWVLj6OT+Vv3+4mvaCUq7u14OnRXegUoRlPa5Mjecf4eEMqH8cf5FDuMUICfbm1b2vu6N9OXR7lvPz4t//at7vJKChlVPcWPDkqxmP/nxSwa1hJeSU+Xo3wquGWyKT0Qn729hoC/bz55OHLaNFUd1jXJUWlFdz61hoO5x5j0aND9SVJ5AIVl1Uw6/v9vP3dPorLKrijf1ueuKqz3hs9rLC0gmc+2cKXW4/gsjC0UxjjBrTl6m4tNHqWOKK4rIKZq/bzj5X7OFZeyVvxd10AACAASURBVD2D2vPcjd0uercRBex65IeDudz1z3W0aR7Axw8O1pBWdYS1lqnvb2Tx9qPMmTiAyzuHe7pKIvVGVmEpbyxL4v31B/BqZHhgaAcevKKDrhB5QHp+CRPnbCDxaAGTh3XgrgHtaBca4OlqST3149++n08jnr2260U/vwJ2PbM6KZOJszfQs00w704aQICvhjOq7d5Yuoe/LtnNb67ryuTLO3i6OiL1UkpWMa98s4tFPxwmJNCXR4d34q6B7dTH9yJJSi9kwqw4sovKmD6+L8O7RHi6SiI16kwBu/aMdSLVNqRTGNPu7MOmlBwefm/jSRM4SO2zZEcaf12ym1subc0DwzRLo0hNaRcawLQ7L+XzR4fStWUQz3+xg8tfXs7M7/dzrKzS09Wr1+KTs7nt7TWUVlTy0YODFK6lwVMLdh32YVwKz3y6lZt6t+Jvd/TRaBS10J60Am6ZvoYO4YF8/OBgtaSJXERr92Yxbeke1u7LIqyJL1Mu78D4Qe111c9hX287ys8/3ESrZo2ZM7E/7UN1f4k0DGdqwda7TB02bkA7corL+fPXiTQP8OF3N3WvFeNCSpW84nImz4vH38eLf9zTT+Fa5CIb3DGUwR1DidufzbSle/jjl4m8/d0+Jg/rwD2D29NEswVesHlrk3lu0XZ6t2nGzAmxhDbx83SVRGoFvbvUcQ9d0YGc4jJmrNxH80BfnriqepOWWGtJPFpwfOr2mJZBvDCmhwK6QyoqXTw6fyOHco/x4ZRBtAxu7OkqiTRYA6JCeO+BgSQcyGba0iT+/HUi/1i5lweGRnHvZZE01c2Q58zlsry8eBdvf7eXq7pG8MadfWvlVNYinqKAXccZY3j22hhyisr427d7aB7gy4TLIk+5bUl5JWv2ZrJ0Z1WoPpxXAkC7kADiD+Rwadvm3NqvzUWsff318uJdrNqTyUtje3p8IHwRqdKvfQhz7x/A5oO5vLF0D698s5sZK/dx/9AoJg6Jcnz+gvqqrMLF059sYeGmQ9w1sB3P39S9Vk1fLVIbqA92PVFR6eLh9zeyZEcar4/rw5g+rYGqCWqWJaazLDGdNXszKSl3EeDrxbDoMEbGtODKLuGENvHjzhnr2HEkn69+Poy2IRpS6UL8e9MhnvhoM/cObs/zY3p4ujoichpbU/OYtmwPS3akEeTnzV2D2nFFdDh92zdXl67TKCgp5+H3NvJ9Uia/uqYzjwzvpCuf0mBpmL4GoqS8kgmz4kg4kMOdA9oRfyCHnUfyAWgb0piRMS0Y2TWCAVEh/zXY/8HsYq59fRXdWjZl/pRBNT5pTn21fFc6D72bQJ+2zXjvgYH4qFVHpNbbfjiPN5Ym8c2Oo7gs+HgZerdpxqAOoQzsEEK/9s11YySQll/CfbM3sCetgD+N7cnPYtt6ukoiHqWA3YAUlJRz1z/Xs+NIPrHtmzOyawQjYiLoGN7krK0MCxJS+dW/fuDp0TE8fGXHi1Tj+uPj+IM8++lWurQI4t1JA3Szj0gdk19STnxyNuv3ZbNufzbbDuVR6bJ4NzL0bBPMwKiqwB3bvnmDmsTGWsv2w/k8+G4CucVlTB/fjys0WZaIAnZDU1pRSVmF65w/AH6cbfDbnWksnDqEHq2Da6iG9Yu1lmlLk3jt290Miw5j+t19G9SHr0h9VVhaQcKBHNbvy2L9/my2pOZSXmlpZKBH62Bu6NWSycM61KsuEtZaDmYfY+uhPLYdzmPboapHTnE5YU38mDOxvz4bRNwUsKXacorKGPW3lQQ39uHzx4bWq36IJeWVzFq9n/fXpTC4Yyj/e31XmgX4XtAxKypd/L/PtjE/7iBj+7bmpbG98PVWtxCR+uhYWSUbU6oC98o9mWw+mMvLt/bi9v51s6uEy2VJzipi66E8th/OZ2tqHtsP55FfUgFUdZXpckkQPVoF0711MKO6tSCiqb+Hay1Seyhgyzn5bncGE2bFMXFIJM/d2N3T1blglS7LgoSDvLZkD0fzS+jbrhlbUvNoFuDDczd254ZeLc+rBaq4rIJHP9jEssR0HhnekV9d06VetWSJyOlVuizj31nP5oO5fPH4UDqGN/F0laqttKKS//fvbXy59SiFpVVh2te7EV0vCaJH62B6tA6mZ+tgols0+a/7dUTk/yhgyzl77rNtzF17gHcnDWBYdN3sa2etZenOdP78dSJ70gvp07YZz14bw8AOoew4nM8zn25hS2oeI2MieOHmHrRqVv2xqjMLS5k0ZwNbD+Xx/JgejB/UvgZfiYjURkfzSrj29ZW0DG7MwkcuqxNhtLisggffTWDVnkx+1q8N/SND6OEO07opW+TcKGDLOTtWVskNb6yisLSCxU9cfsFdKS62jSk5vPRlInHJ2USFBfLUqC6M7nHJSS3MlS7L7NX7+es3u2lk4OlrYxg/sP1Zp5xPzixiwuw40vJLmDbuUq7pfklNvxwRqaW+3ZHGA/PimTQ0iv93QzdPV+eM8orLmTgnjs0Hc3np1l7crlFARC6IAracl22H8rj5zdWM6n4Jf7/r0jrR/WFfRiF/WbyLr7YdJayJHz+/Kppx/duesWXmYHYxv164lVV7MunXvjkvje1JdIugU267+WAuk+ZswGUtM+/rT992zWvqpYhIHfHjFb/ZE/szvEuEp6tzShkFpdwzcz17Mwp5485LGd2jpaerJFLnKWDLeXtzeRJ/WbyLV2/vzdi+tXeWx/SCEl7/dg8fbjiIv3cjplzekQeGRRHoV72xa621LNx0iOe/2EFRaQWPDO/Ew1d2POmS79KdaTz6wSbCgnyZO3EAHepQn0sRqTkl5ZXc/OZqMgpK+eqJYUQE1a4bAVNzirlnZhxH80r4xz39uFxD7Ik4QgFbzlulyzJuxloSjxTw1RPDaNPcs7M8llZUklFQSnpBKen5pWQUlLAvs4iPNhykrMLFXQPb8diIaMKDzm8M6szCUl74YgefbT5MdEQTXrq1F/3aN2d+XAq/WbiV7q2CmXVf//M+vojUT3vSCrjx79/TPzKEuRMHnLWr2cWSlF7IPTPXU1hawZyJ/enXPsTTVRKpNxSw5YIcn+WxVVPmT675WR63pOaScCCH9IJS0vJLqgJ1finpBSXkFJf/1/ZejQyju1/Cr0Z1ISos0JE6LE9M5zcLt3Ikv4TLOoayOimLK7uE8+ZdfavdKi4iDcv76w/wm4Xb+PV1MUy5/Pwn68opKuN/P9tGanYxdw1sx5g+rc9ryNRth/KYMCsOY2De/QPp1qrpeddJRP6bArZcsH/FH+TJBVt45toYHrqi5mZ53JKay9jpa6hwz54WEeRHeFN/IoL8iAjyo8WPy039iAjyJ6KpH6GBfjUS+gtLK3hl8S7mrk3mtr5t+OPYnrrLXkROy1rLw+9VTdb16dTL6NWm2TkfI25/Nj//cBNZhWW0Dw1gT3ohoYG+jB/UnnsGtyesmjPEbkjO5v7ZG2ja2Id3J6lLm0hNUMCWC/bjB8fSxDT+/cgQurdyfiavY2WVXP/GKopLK/lk6mW0bOpfKy6z5hWX07Sxd524yVNEPCu3uIxrX1+Fn3cjvnh8GE2qecWr0mWZvrxqRth2IQH8/a6+dG/VlLX7spi5aj9LE9Px9W7ELX1ac//QKLpccuobsQFW7ErnofcSaNWsMe9NGnhOQ5CKSPUpYIsjst2zPDYP8GHRo87P8vjbz7Yxb+0B3n9gIEM6hTl6bBGRi2X9vizu/Oc6br60Na/e3ues26fnl/DER5tZszeLMX1a8eItPf8rmO/NKGT26v0sSEilpNzFsOgwJg2N4orO4Sd9+f/PliM88dEmoiOCmDdpQLVbvEXk3J0pYOt6t1RbSKAvf7mtF7vTCvnDf3bg5JezFbvSmbf2APcPiVK4FpE6bWCHUB4dEc2nGw/x702Hzrjtyt0ZXDdtFRtTcnj51l787Y4+p2z17hjehD/c3JO1z4zkyVFd2HW0gPtmb+Ca11YyPy6FkvJKPtqQwmPzN9KnbTPmTxmkcC3iQWrBlnP2xy93MmPlPv73+q48MKzDBR+vplvGRUQutopKF+NmrCPxaAFfPj6MdqEnj8BUXuni1SW7eWvFXjq3aMLf7+pL59OMv38qZRUuvthymHdW7WfHkXyCG/uQd6ycKzqH8/b4fjT21fuoSE1TFxFxlMtleWz+Jv6z9Qhv3HkpN/Zudd7HOrFv92ePDNVd7iJSb6TmVI3A1CG8CQseGnz8JulDucd4fP4mEg7kcOeAtvz2hu7nHYittazbl83s1ftpHuDLCzf3wNdbF6dFLoYzBWyNNybnrFEjw19v7016QQm//PgHIoL8GNgh9LyOtSAhla+3H+XZa2MUrkWkXmnTPICXxvbikQ828uqS3Tw9OoZvth/lyQVbqHRZpt15KTddQAMFgDGGwR1DGdzx/N6DRaRm6GuunBd/Hy/+eW8sbUMaM3lePHvSCs75GClZxfxu0XYGRoU40tVERKS2ub5XS8b1b8vb3+1l6vsJTHk3gXYhAXzx2NALDtciUnspYMt5axbgy5yJA/Dz8eK+2RtIyy+p9r6VLssvPt5MI1PVGl7Tk9eIiHjKb2/sRoewQL7cepT7h0Sx4OHBRDo0KZaI1E4K2HJB2oYEMPu+/uQUlzFx9gYKSyuqtd/b3+0l/kAOz9/c3ePTr4uI1KQAX28+mDyIT6dexm9v7Iaft25AFKnvFLDlgvVoHcz0u/uyK62Ah99LoLzSdcbtt6bm8dqS3VzfqyU392l9kWopIuI5LZr607ddc09XQ0QuEgVsccSVXSL40y09WbUnk2c/3XraMbKPlVXyxEebCGvix4s399DsiCIiIlLvaBQRcczt/dtyKPcYry/dQ6tmjfnF1Z3/a5uXvtrJ3owi3ps0kGYBvh6opYiIiEjNUsAWRz1xVTSHc48xbekeWgX7M25Au+Prvtudwdy1B5g4JJKh0ZqtUUREROonBWxxlDGGP47tSVpBKb/59zZaBPszvEsEOUVlPPmvH4iOaMLTo2M8XU0RERGRGqM+2OI4H69GTL+7LzGXBPHI+xvZmprHrxduJae4jL+N66Op0EVERKReU8CWGtHEz5vZ9/WneYAvd8xYy1fbjvLLa7rQvVWwp6smIiIiUqMUsKXGRDT1Z+79/fHxasSgDiFM1myNIiIi0gCoD7bUqE4RQax8cjiNfb00W6OIiIg0CArYUuOCA3w8XQURERGRi0ZdREREREREHKSALSIiIiLiIAVsEREREREHKWCLiIiIiDhIAVtERERExEEK2CIiIiIiDlLAFhERERFxkAK2iIiIiIiDqhWwjTHNjDELjDGJxpidxpjBxpgQY8wSY8we98/m7m2NMWaaMSbJGLPFGNP3hONMcG+/xxgz4YTyfsaYre59phljjLv8lOcQEREREamtqtuC/TrwtbU2BugN7ASeAZZaa6OBpe7nANcC0e7HFOAtqArLwHPAQGAA8NwJgfktYPIJ+412l5/uHCIiIiIitdJZA7YxJhi4HJgJYK0ts9bmAmOAue7N5gI3u5fHAPNslXVAM2NMS2AUsMRam22tzQGWAKPd65paa9dZay0w7yfHOtU5RERERERqpeq0YEcBGcBsY8wmY8w7xphAoIW19oh7m6NAC/dya+DgCfunusvOVJ56inLOcA4RERERkVqpOgHbG+gLvGWtvRQo4iddNdwtz9b56lXvHMaYKcaYeGNMfEZGRk1WQ0RERETkjKoTsFOBVGvtevfzBVQF7jR39w7cP9Pd6w8BbU/Yv4277EzlbU5RzhnOcRJr7Qxrbay1NjY8PLwaL0lEREREpGacNWBba48CB40xXdxFI4EdwCLgx5FAJgCfuZcXAfe6RxMZBOS5u3ksBq4xxjR339x4DbDYvS7fGDPIPXrIvT851qnOISIiIiJSK3lXc7vHgPeNMb7APmAiVeH8Y2PMJOAAcLt72y+B64AkoNi9LdbabGPMC8AG93bPW2uz3ctTgTlAY+Ar9wPgpdOcQ0RERESkVjJVXZvrj9jYWBsfH+/paoiIiIhIPWaMSbDWxp5qnWZyFBERERFxkAK2iIiIiIiDFLBFRERERBykgC0iIiIi4iAFbBERERERBylgi4iIiIg4SAFbRERERMRBCtgiIiIiIg5SwBYRERERcZACtoiIiIiIgxSwRUREREQcpIAtIiIiIuIgBWwREREREQcpYIuIiIiIOEgBW0RERETEQQrYIiIiIiIOUsAWEREREXGQAraIiIiIiIMUsEVEREREHKSALSIiIiLiIAVsEREREREHKWCLiIiIiDhIAVtERERExEEK2CIiIiIiDlLAFhERERFxkAK2iIiIiIiDFLBFRERERBykgC0iIiIi4iAFbBERERERBylgi4iIiIg4SAFbRERERMRBCtgiIiIiIg5SwBYRERERcZACtoiIiIiIgxSwRUREREQcpIAtIiIiIuIgBWwREREREQcpYIuIiIiIOEgBW0RERETEQQrYIiIiIiIOUsAWEREREXGQAraIiIiIiIMUsEVEREREHKSALSIiIiLiIAVsEREREREHKWCLiIiIiDhIAVtERERExEEK2CIiIiIiDlLAFhERERFxkAK2iIiIiIiDqhWwjTHJxpitxpjNxph4d1mIMWaJMWaP+2dzd7kxxkwzxiQZY7YYY/qecJwJ7u33GGMmnFDez338JPe+5kznEBERERGprc6lBXu4tbaPtTbW/fwZYKm1NhpY6n4OcC0Q7X5MAd6CqrAMPAcMBAYAz50QmN8CJp+w3+iznENEREREpFa6kC4iY4C57uW5wM0nlM+zVdYBzYwxLYFRwBJrbba1NgdYAox2r2tqrV1nrbXAvJ8c61TnEBERERGplaobsC3wjTEmwRgzxV3Wwlp7xL18FGjhXm4NHDxh31R32ZnKU09RfqZznMQYM8UYE2+Mic/IyKjmSxIRERERcZ53Nbcbaq09ZIyJAJYYYxJPXGmttcYY63z1qncOa+0MYAZAbGxsjdZDRERERORMqtWCba095P6ZDiykqg91mrt7B+6f6e7NDwFtT9i9jbvsTOVtTlHOGc4hIiIiIlIrnTVgG2MCjTFBPy4D1wDbgEXAjyOBTAA+cy8vAu51jyYyCMhzd/NYDFxjjGnuvrnxGmCxe12+MWaQe/SQe39yrFOdQ0RERESkVqpOF5EWwEL3yHnewAfW2q+NMRuAj40xk4ADwO3u7b8ErgOSgGJgIoC1NtsY8wKwwb3d89babPfyVGAO0Bj4yv0AeOk05xARERERqZVM1cAd9UdsbKyNj4/3dDVEREREpB4zxiScMHz1STSTo4iIiIiIgxSwRUREREQcpIAtIiIiIuIgBWwREREREQcpYIuIiIiIOEgBW0RERETEQQrYIiIiIiIOUsAWEREREXGQAraIiIiIiIMUsEVEREREHKSALSIiIiLiIAVsEREREREHKWCLiIiIiDhIAVtERERExEEK2CIiIiIiDlLAFhERERFxkAK2iIiIiIiDFLBFRERERBykgC0iIiIi4iAFbBERERERBylgi4iIiIg4SAFbRERERMRBCtgiIiIiIg5SwBYRERERcZACtoiIiIiIgxSwRUREREQcpIAtIiIiIuIgBWwREREREQcpYIuIiIiIOEgBW0RERETEQQrYIiIiIiIOUsAWEREREXGQAraIiIiIiIMUsEVEREREHKSALSIiIiLiIAVsEREREREHKWCLiIiIiDhIAVtERERExEEK2CIiIiIiDlLAFhERERFxkAK2iIiIiIiDFLBFRERERBykgC0iIiIi4iAFbBERERERBylgi4iIiIg4SAFbRERERMRBCtgiIiIiIg5SwBYRERERcZACtoiIiIiIgxSwRUREREQcVO2AbYzxMsZsMsZ84X4eZYxZb4xJMsZ8ZIzxdZf7uZ8nuddHnnCMZ93lu4wxo04oH+0uSzLGPHNC+SnPISIiIiJSW51LC/bPgZ0nPP8z8Jq1thOQA0xyl08Cctzlr7m3wxjTDRgHdAdGA9Pdod0LeBO4FugG3One9kznEBERERGplaoVsI0xbYDrgXfczw0wAljg3mQucLN7eYz7Oe71I93bjwE+tNaWWmv3A0nAAPcjyVq7z1pbBnwIjDnLOUREREREaqXqtmD/DXgKcLmfhwK51toK9/NUoLV7uTVwEMC9Ps+9/fHyn+xzuvIzneMkxpgpxph4Y0x8RkZGNV+SiIiIiIjzzhqwjTE3AOnW2oSLUJ/zYq2dYa2NtdbGhoeHe7o6IiIiItKAeVdjmyHATcaY6wB/oCnwOtDMGOPtbmFuAxxyb38IaAukGmO8gWAg64TyH524z6nKs85wDhERERGRWumsLdjW2mettW2stZFU3aS4zFp7N7AcuM292QTgM/fyIvdz3OuXWWutu3yce5SRKCAaiAM2ANHuEUN83edY5N7ndOcQEREREamVLmQc7KeBXxhjkqjqLz3TXT4TCHWX/wJ4BsBaux34GNgBfA08Yq2tdLdOPwospmqUko/d257pHCIiIiIitZKpaiiuP2JjY218fLynqyEiIiIi9ZgxJsFaG3uqdZrJUURERETEQQrYIiIiIiIOUsAWEREREXGQAraIiIiIiIMUsEVEREREHKSALSIiIiLiIAVsEREREREHKWCLiIiIiDhIAVtERERExEEK2CIiIiIiDlLAFhERERFxkAK2iIiIiIiDFLBFRERERBykgC0iIiIi4iAFbBERERERBylgi4iIiIg4SAFbRERERMRBCtgiIiIiIg5SwBYRERERcZACtoiIiIiIgxSwRUREREQcpIAtIiIiIuIgBWwREREREQcpYIuIiIiIOEgBW0RERETEQQrYIiIiIiIOUsAWEREREXGQAraIiIiIiIMUsEVEREREHKSALSIiIiLiIAVsEREREREHKWCLiIiIiDhIAVtERERExEEK2CIiIiIiDlLAFhERERFxkAK2iIiIiIiDFLBFRERERBykgC0iIiIi4iAFbBERERERBylgi4iIiIg4SAFbRERERMRBCtgiIiIiIg5SwBYRERERcZACtoiIiIiIgxSwRUREREQcpIAtIiIiIuIgBWwREREREQcpYIuIiIiIOEgBW0RERETEQQrYIiIiIiIOOmvANsb4G2PijDE/GGO2G2N+7y6PMsasN8YkGWM+Msb4usv93M+T3OsjTzjWs+7yXcaYUSeUj3aXJRljnjmh/JTnEBERERGprarTgl0KjLDW9gb6AKONMYOAPwOvWWs7ATnAJPf2k4Acd/lr7u0wxnQDxgHdgdHAdGOMlzHGC3gTuBboBtzp3pYznENEREREpFY6a8C2VQrdT33cDwuMABa4y+cCN7uXx7if414/0hhj3OUfWmtLrbX7gSRggPuRZK3dZ60tAz4Exrj3Od05RERERERqpWr1wXa3NG8G0oElwF4g11pb4d4kFWjtXm4NHARwr88DQk8s/8k+pysPPcM5flq/KcaYeGNMfEZGRnVekoiIiIhIjahWwLbWVlpr+wBtqGpxjqnRWp0ja+0Ma22stTY2PDzc09URERERkQbsnEYRsdbmAsuBwUAzY4y3e1Ub4JB7+RDQFsC9PhjIOrH8J/ucrjzrDOcQEREREamVqjOKSLgxppl7uTFwNbCTqqB9m3uzCcBn7uVF7ue41y+z1lp3+Tj3KCNRQDQQB2wAot0jhvhSdSPkIvc+pzuHiIiIiEit5H32TWgJzHWP9tEI+Nha+4UxZgfwoTHmD8AmYKZ7+5nAu8aYJCCbqsCMtXa7MeZjYAdQATxira0EMMY8CiwGvIBZ1trt7mM9fZpziIiIiIjUSqaqobj+iI2NtfHx8Z6uhoiIiIjUY8aYBGtt7KnWaSZHEZH/394dhtpd33cc/3yJymRdUWuWSZLNjgVGYJttL5rRPnDCbHRl6YMhLdsMRZoH7cBCx8j6RGYpuCfdJhRBVjFC107WdspQXEiF7onOa+sarStmpWKCmozY2lFocfvuwf3Ljllyk9789Nx7+nrB5Zz/7/8/9/+TH5688+d/TgBgIIENAAADCWwAABhIYAMAwEACGwAABhLYAAAwkMAGAICBBDYAAAwksAEAYCCBDQAAAwlsAAAYSGADAMBAAhsAAAYS2AAAMJDABgCAgQQ2AAAMJLABAGAggQ0AAAMJbAAAGEhgAwDAQAIbAAAGEtgAADCQwAYAgIEENgAADCSwAQBgIIENAAADCWwAABhIYAMAwEACGwAABhLYAAAwkMAGAICBBDYAAAwksAEAYCCBDQAAAwlsAAAYSGADAMBAAhsAAAYS2AAAMJDABgCAgQQ2AAAMJLABAGAggQ0AAAMJbAAAGEhgAwDAQAIbAAAGEtgAADCQwAYAgIEENgAADCSwAQBgIIENAAADnTWwq2p7VT1aVd+uqmeq6tZp/LKqOlhVz02Pl07jVVV3VtWRqvpWVb175nftnY5/rqr2zoy/p6oOT6+5s6pqtXMAAMB6dS5XsF9L8snu3plkV5KPV9XOJPuTHOruHUkOTdtJckOSHdPPviR3JSuxnOS2JNckuTrJbTPBfFeSj868bvc0fqZzAADAunTWwO7uF7v7G9PzHyZ5NsnWJHuSHJgOO5Dkg9PzPUnu6xWPJbmkqq5I8v4kB7v7ZHe/kuRgkt3Tvrd392Pd3UnuO+V3ne4cAACwLv1U92BX1ZVJ3pXk8SRbuvvFaddLSbZMz7cmeWHmZUensdXGj55mPKuc49R57auq5apaPnHixE/znwQAAEOdc2BX1duSfDnJJ7r71dl905XnHjy3N1jtHN19d3cvdffS5s2b38xpAADAqs4psKvqwqzE9Re6+yvT8MvT7R2ZHo9P48eSbJ95+bZpbLXxbacZX+0cAACwLp3Lt4hUks8neba7Pzuz68Ekr38TyN4kD8yM3zx9m8iuJD+YbvN4JMn1VXXp9OHG65M8Mu17tap2Tee6UqF6bAAACaJJREFU+ZTfdbpzAADAunTBORzz3iR/nORwVT01jX0qyR1J7q+qW5I8n+Smad9DSW5MciTJj5J8JEm6+2RVfTrJE9Nxt3f3yen5x5Lcm+TiJA9PP1nlHAAAsC7Vyq3Ni2NpaamXl5fnPQ0AABZYVT3Z3Uun2+dfcgQAgIEENgAADCSwAQBgIIENAAADCWwAABhIYAMAwEACGwAABhLYAAAwkMAGAICBBDYAAAwksAEAYCCBDQAAAwlsAAAYSGADAMBAAhsAAAYS2AAAMJDABgCAgQQ2AAAMJLABAGAggQ0AAAMJbAAAGEhgAwDAQAIbAAAGEtgAADCQwAYAgIEENgAADCSwAQBgIIENAAADCWwAABhIYAMAwEACGwAABhLYAAAwkMAGAICBBDYAAAwksAEAYCCBDQAAAwlsAAAYSGADAMBAAhsAAAYS2AAAMJDABgCAgQQ2AAAMJLABAGAggQ0AAAMJbAAAGEhgAwDAQAIbAAAGEtgAADCQwAYAgIHOGthVdU9VHa+qp2fGLquqg1X13PR46TReVXVnVR2pqm9V1btnXrN3Ov65qto7M/6eqjo8vebOqqrVzgEAAOvZuVzBvjfJ7lPG9ic51N07khyatpPkhiQ7pp99Se5KVmI5yW1JrklydZLbZoL5riQfnXnd7rOcAwAA1q2zBnZ3fz3JyVOG9yQ5MD0/kOSDM+P39YrHklxSVVckeX+Sg919srtfSXIwye5p39u7+7Hu7iT3nfK7TncOAABYt9Z6D/aW7n5xev5Ski3T861JXpg57ug0ttr40dOMr3YOAABYt877Q47TleceMJc1n6Oq9lXVclUtnzhx4s2cCgAArGqtgf3ydHtHpsfj0/ixJNtnjts2ja02vu0046ud4//p7ru7e6m7lzZv3rzG/yQAADh/aw3sB5O8/k0ge5M8MDN+8/RtIruS/GC6zeORJNdX1aXThxuvT/LItO/Vqto1fXvIzaf8rtOdAwAA1q0LznZAVX0xybVJLq+qo1n5NpA7ktxfVbckeT7JTdPhDyW5McmRJD9K8pEk6e6TVfXpJE9Mx93e3a9/cPJjWfmmkouTPDz9ZJVzAADAulUrtzcvjqWlpV5eXp73NAAAWGBV9WR3L51un3/JEQAABhLYAAAwkMAGAICBBDYAAAwksAEAYCCBDQAAAwlsAAAYSGADAMBAAhsAAAYS2AAAMJDABgCAgQQ2AAAMJLABAGAggQ0AAAMJbAAAGEhgAwDAQAIbAAAGEtgAADCQwAYAgIEENgAADCSwAQBgIIENAAADCWwAABhIYAMAwEACGwAABhLYAAAwkMAGAICBBDYAAAwksAEAYCCBDQAAAwlsAAAYSGADAMBAAhsAAAYS2AAAMJDABgCAgQQ2AAAMJLABAGAggQ0AAAMJbAAAGEhgAwDAQAIbAAAGEtgAADCQwAYAgIEENgAADCSwAQBgIIENAAADCWwAABhIYAMAwEACGwAABhLYAAAw0LoP7KraXVXfqaojVbV/3vMBAIDVrOvArqpNST6X5IYkO5N8uKp2zndWAABwZus6sJNcneRId3+3u3+S5EtJ9sx5TgAAcEYXzHsCZ7E1yQsz20eTXDOnuZzZw/uTlw7PexYAAD97fuk3khvumPcs3mC9X8E+J1W1r6qWq2r5xIkT854OAAA/w9b7FexjSbbPbG+bxt6gu+9OcneSLC0t9VsztRnr7G9NAADMz3q/gv1Ekh1V9c6quijJh5I8OOc5AQDAGa3rK9jd/VpV/UmSR5JsSnJPdz8z52kBAMAZrevATpLufijJQ/OeBwAAnIv1fosIAABsKAIbAAAGEtgAADCQwAYAgIEENgAADCSwAQBgIIENAAADCWwAABhIYAMAwEACGwAABhLYAAAwkMAGAICBBDYAAAwksAEAYCCBDQAAAwlsAAAYSGADAMBA1d3znsNQVXUiyfNzOPXlSf5zDuflrWF9F5e1XVzWdrFZ38W1Udb2V7p78+l2LFxgz0tVLXf30rznwZvD+i4ua7u4rO1is76LaxHW1i0iAAAwkMAGAICBBPY4d897AryprO/israLy9ouNuu7uDb82roHGwAABnIFGwAABhLYA1TV7qr6TlUdqar9854Pa1dV91TV8ap6embssqo6WFXPTY+XznOOrE1Vba+qR6vq21X1TFXdOo1b3wVQVT9XVf9aVf82re9fTOPvrKrHp/fnv6+qi+Y9V9amqjZV1Ter6p+mbWu7IKrqe1V1uKqeqqrlaWxDvzcL7PNUVZuSfC7JDUl2JvlwVe2c76w4D/cm2X3K2P4kh7p7R5JD0zYbz2tJPtndO5PsSvLx6f9V67sYfpzkuu7+rSRXJdldVbuS/GWSv+ruX0vySpJb5jhHzs+tSZ6d2ba2i+V3uvuqma/n29DvzQL7/F2d5Eh3f7e7f5LkS0n2zHlOrFF3fz3JyVOG9yQ5MD0/kOSDb+mkGKK7X+zub0zPf5iVP6i3xvouhF7xX9PmhdNPJ7kuyT9M49Z3g6qqbUl+L8nfTtsVa7voNvR7s8A+f1uTvDCzfXQaY3Fs6e4Xp+cvJdkyz8lw/qrqyiTvSvJ4rO/CmG4heCrJ8SQHk/xHku9392vTId6fN66/TvJnSf5n2n5HrO0i6ST/XFVPVtW+aWxDvzdfMO8JwEbS3V1VvnpnA6uqtyX5cpJPdPerKxfCVljfja27/zvJVVV1SZKvJvn1OU+JAarqA0mOd/eTVXXtvOfDm+J93X2sqn4xycGq+vfZnRvxvdkV7PN3LMn2me1t0xiL4+WquiJJpsfjc54Pa1RVF2Ylrr/Q3V+Zhq3vgunu7yd5NMlvJ7mkql6/mOT9eWN6b5Lfr6rvZeU2zOuS/E2s7cLo7mPT4/Gs/OX46mzw92aBff6eSLJj+jTzRUk+lOTBOc+JsR5Msnd6vjfJA3OcC2s03bP5+STPdvdnZ3ZZ3wVQVZunK9epqouT/G5W7rN/NMkfTIdZ3w2ou/+8u7d195VZ+TP2a939h7G2C6Gqfr6qfuH150muT/J0Nvh7s39oZoCqujEr94dtSnJPd39mzlNijarqi0muTXJ5kpeT3JbkH5Pcn+SXkzyf5KbuPvWDkKxzVfW+JP+S5HD+7z7OT2XlPmzru8FV1W9m5YNQm7Jy8ej+7r69qn41K1c9L0vyzSR/1N0/nt9MOR/TLSJ/2t0fsLaLYVrHr06bFyT5u+7+TFW9Ixv4vVlgAwDAQG4RAQCAgQQ2AAAMJLABAGAggQ0AAAMJbAAAGEhgAwDAQAIbAAAGEtgAADDQ/wICZpOfB5J07wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x648 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzR4T3zKuR_F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
